{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10220210,"sourceType":"datasetVersion","datasetId":6317941},{"sourceId":10221151,"sourceType":"datasetVersion","datasetId":6318559},{"sourceId":10228384,"sourceType":"datasetVersion","datasetId":6323908},{"sourceId":10228640,"sourceType":"datasetVersion","datasetId":6324124},{"sourceId":10229037,"sourceType":"datasetVersion","datasetId":6324405},{"sourceId":10229425,"sourceType":"datasetVersion","datasetId":6324700},{"sourceId":10229470,"sourceType":"datasetVersion","datasetId":6324736},{"sourceId":10229672,"sourceType":"datasetVersion","datasetId":6324871}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# General\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport re\nimport os\nimport tqdm\nimport gc\nimport spacy\n# Load spacy model for lemmatization\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import regexp_tokenize\n\n# Models\n\nimport torch\nfrom nltk.tokenize import regexp_tokenize\n# Evaluation metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:00.588700Z","iopub.execute_input":"2024-12-17T23:58:00.589460Z","iopub.status.idle":"2024-12-17T23:58:06.400657Z","shell.execute_reply.started":"2024-12-17T23:58:00.589421Z","shell.execute_reply":"2024-12-17T23:58:06.399705Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# # General\n# import numpy as np\n# import pandas as pd\n\n# # Preprocessing\n# from gensim.models import FastText\n# import os\n# import json\n# import pickle\n# import psutil\n# import numpy as np\n# import pandas as pd\n# import tqdm\n# from typing import Literal\n\n# #* Configurations\n# #* Folder Paths\n# DATASET_PATH = \"../../data/dataset\" # Local\n# DATASET_PATH = \"/kaggle/input/dataset-with-is-final-labels/train_95_with_NER_and_IS_labels.parquet\" # Kaggle\n\n# OUTPUT_ROOT_PATH = \"../../data/saved\" # Local\n# OUTPUT_ROOT_PATH = \"/kaggle/working\" # Kaggle\n\n# PROCESSED_DATA_PATH = OUTPUT_ROOT_PATH + \"/data\"\n# FEATURES_PATH = OUTPUT_ROOT_PATH + \"/features\"\n# MODELS_PATH = OUTPUT_ROOT_PATH + \"/models\"\n\n# #* Common Variables\n# token_pattern=r\"(?u)\\b\\w+(?:'\\w+)?(?:-\\w+)*\\b\"\n\n# def run_config():\n#     #* Pandas\n#     pd.set_option('display.max_colwidth', 1000) # Show all content of the cells\n#     # pd.reset_option('display.max_colwidth') # Undo with \n    \n#     #* Config tqdm for pandas\n#     tqdm.tqdm.pandas()\n\n#     #* Output Folders\n#     os.makedirs(OUTPUT_ROOT_PATH, exist_ok=True)\n#     os.makedirs(FEATURES_PATH, exist_ok=True)\n#     os.makedirs(MODELS_PATH, exist_ok=True)\n#     os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n#     # os.rmdir(OUTPUT_ROOT_PATH)\n#     # os.rmdir(FEATURES_PATH)\n#     # os.rmdir(MODELS_PATH)\n#     # os.rmdir(PROCESSED_DATA_PATH)\n# run_config()\n\n# import os\n# import json\n# import pickle\n# import pyarrow as pa\n# import pyarrow.dataset as pda\n# import pyarrow.parquet as pq\n# import glob\n# import psutil\n# import numpy as np\n# import pandas as pd\n# from typing import Literal\n\n\n# types = Literal[\"model\", \"feature\", \"processed\"]\n\n# #* General\n# def file_exists(path):\n#     return os.path.exists(path)\n\n# def add_to_path(path: str, type: types | None = None):\n#     if type is not None:\n#         if type == \"model\":\n#             path = MODELS_PATH + \"/\" + path\n#         elif type == \"feature\":\n#             path = FEATURES_PATH + \"/\" + path\n#         elif type == \"processed\":\n#             path = PROCESSED_DATA_PATH + \"/\" + path\n#     return path\n\n# #* Memory Management & Performance\n# def memory_usage():\n#     process = psutil.Process(os.getpid())\n#     return (process.memory_info().rss / 1024 ** 2)\n\n\n# #* Save & Load functions\n# def save_pickle(path: str, obj, type: types | None = None):\n#     path = add_to_path(path, type)\n#     with open (path, 'wb') as f:\n#         pickle.dump(obj, f)\n\n# def load_pickle(path: str, type: types | None = None):\n#     path = add_to_path(path, type)\n#     with open(path, 'rb') as f:\n#         return pickle.load(f)\n    \n# def save_parquet(path: str, obj, type: types | None = None):\n#     path = add_to_path(path, type)\n#     obj.to_parquet(path, engine='pyarrow', compression='snappy')\n\n# def load_parquet(path: str, type: types | None = None):\n#     path = add_to_path(path, type)\n#     return pd.read_parquet(path, engine='pyarrow')\n    \n# def save_np(path: str, obj, type: types | None = None, allow_pickle=True):\n#     path = add_to_path(path, type)\n#     np.save(path, obj, allow_pickle=allow_pickle)\n\n# def load_np(path: str, type: types | None = None, allow_pickle=True):\n#     path = add_to_path(path, type)\n#     return np.load(path, allow_pickle=allow_pickle)\n\n# def save_dict_to_json(path: str, obj, type: types | None = None):\n#     path = add_to_path(path, type)\n#     # Convert ndarray to list\n#     for key, value in obj.items():\n#         if isinstance(value, np.ndarray):\n#             obj[key] = value.tolist()\n\n#     with open(path, 'w') as f:\n#         json.dump(obj, f)\n\n# def load_json_to_dict(path: str, type: types | None = None):\n#     path = add_to_path(path, type)\n#     with open(path, 'r') as f:\n#         return json.load(f)\n\n# def load_json(filename: str, cols: list[str] | None = None):\n#     \"\"\"\n#     Load a json file into a pandas DataFrame.\n#     * This function is useful (for some reason) for loading the large dataset files.\n    \n#     filename: str\n#         The name of the file to load.\n#     cols: list[str] | None\n#         The columns to load. If None, load all columns.\n#     return: pd.DataFrame\n#         The DataFrame containing the data from the json file.\n#     \"\"\"\n#     all_cols = True if cols is None else False\n#     data = []\n\n#     with open(filename, encoding='latin-1') as f:\n#         line = f.readline()\n#         f.seek(0) # Go back to the beginning of the file\n#         doc = json.loads(line)\n#         if all_cols:\n#             cols = list(doc.keys())\n        \n#         for line in f:\n#             doc = json.loads(line)\n#             lst = [doc[col] for col in cols]\n#             data.append(lst)\n\n#     df = pd.DataFrame(data=data, columns=cols)\n#     return df\n\n\n# def process_parquet_in_chunks(input_file: str, output_file: str, chunk_size: int, preprocess_function: callable, args: tuple = (), merge_chunks: bool=True):\n#     \"\"\"\n#     Process a large Parquet file in chunks, applying a preprocessing function to each row, \n#     and save the processed chunks as new Parquet files. Optionally merge the processed chunks.\n#     Source: https://blog.clairvoyantsoft.com/efficient-processing-of-parquet-files-in-chunks-using-pyarrow-b315cc0c62f9\n\n#     Parameters:\n#     - input_file (str): Path to the input Parquet file.\n#     - output_file (str): Path to save the processed Parquet file.\n#     - chunk_size (int): Number of rows to process per chunk.\n#     - preprocess_function (function): Function to apply to each row.\n#     - merge_chunks (bool): Whether to merge the processed chunks into a single Parquet file (default: True).\n\n#     Returns:\n#     - None\n#     \"\"\"\n\n#     parquet_file = pq.ParquetFile(input_file) # Dataframe which does not fit into system memory\n\n#     for i, batch in enumerate(parquet_file.iter_batches(batch_size=chunk_size)):\n#         df = batch.to_pandas()\n#         # Process the chunk (batch)\n#         processed_chunk = df.progress_apply(preprocess_function, args=args, axis=1)\n\n#         # Save the processed chunk to a new Parquet file\n#         output_chunk = f\"{output_file}_{i}.parquet\"\n#         processed_chunk.to_parquet(output_chunk, engine='pyarrow', compression='snappy')\n#         print(f\"Chunk {i} processed and saved to {output_chunk}\")\n\n#     # Optionally merge processed chunks\n#     if merge_chunks:\n#         print(\"Merging processed chunks into a single Parquet file...\")\n\n#         # Get all processed chunk files\n#         parquet_files = glob.glob(f\"{output_file}_*.parquet\")\n#         # Read and concatenate them into a single DataFrame\n#         final_df = pd.concat([pd.read_parquet(file) for file in parquet_files], ignore_index=True)\n#         # Save the final DataFrame as a single Parquet file\n#         final_df.to_parquet(output_file, engine='pyarrow', compression='snappy')\n#         # Remove the processed chunk files\n#         for file in parquet_files:\n#             os.remove(file)\n\n#         print(f\"Merged file saved to {output_file}\")\n\n\n# def process_pickles_in_chunks(input_file: str, output_file: str, chunk_size: int, preprocess_function: callable, args: tuple = (), merge_chunks: bool=True):\n#     \"\"\"\n#     Process a large pickle file in chunks, applying a preprocessing function to each row, \n#     and save the processed chunks as new pickle files. Optionally merge the processed chunks.\n\n#     Parameters:\n#     - input_file (str): Path to the input pickle file.\n#     - output_file (str): Path to save the processed pickle file.\n#     - chunk_size (int): Number of rows to process per chunk.\n#     - preprocess_function (function): Function to apply to each row.\n#     - merge_chunks (bool): Whether to merge the processed chunks into a single pickle file (default: True).\n\n#     Returns:\n#     - None\n#     \"\"\"\n\n#     # Load the pickle file\n#     with open(input_file, 'rb') as f:\n#         data = pickle.load(f)\n\n#     # Split the data into chunks\n#     chunks = [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n\n#     for i, chunk in enumerate(chunks):\n#         # Process the chunk\n#         processed_chunk = [preprocess_function(row, *args) for row in chunk]\n\n#         # Save the processed chunk to a new pickle file\n#         output_chunk = f\"{output_file}_{i}.pkl\"\n#         with open(output_chunk, 'wb') as f:\n#             pickle.dump(processed_chunk, f)\n\n#         print(f\"Chunk {i} processed and saved to {output_chunk}\")\n\n#     # Optionally merge processed chunks\n#     if merge_chunks:\n#         print(\"Merging processed chunks into a single pickle file...\")\n\n#         # Get all processed chunk files\n#         pickle_files = glob.glob(f\"{output_file}_*.pkl\")\n#         # Read and concatenate them into a single list\n#         final_data = []\n#         for file in pickle_files:\n#             with open(file, 'rb') as f:\n#                 final_data.extend(pickle.load(f))\n#         # Save the final list as a single pickle file\n#         with open(output_file, 'wb') as f:\n#             pickle.dump(final_data, f)\n#         # Remove the processed chunk files\n#         for file in pickle_files:\n#             os.remove(file)\n\n#         print(f\"Merged file saved to {output_file}\")\n\n\n\n\n\n# df_train = pd.read_parquet('/kaggle/input/dataset-with-is-final-labels/train_95_with_NER_and_IS_labels.parquet',columns=['tokenized'])\n# sentences=df_train['tokenized'].tolist()\n# sentences = [sentence.tolist() if isinstance(sentence, np.ndarray) else sentence for sentence in sentences]\n# '''  \n# <class 'list'>\n# <class 'list'>\n# <class 'str'>\n# ['party', 'size', 'dried', 'peppers', 'pizza', 'and', 'a', 'sprite']\n# '''\n# # Verify the format\n# print(type(sentences))         # Should be list\n# print(type(sentences[0]))      # Should be list\n# print(type(sentences[0][0]))   # Should be str\n# print(sentences[0])            # Check the first sentence\n\n\n\n# from gensim.models import FastText\n# model_name = \"/fast_text_model.bin\"\n# update_model = True\n# if update_model or not os.path.exists(MODELS_PATH + model_name):\n#     print(f\"Creating '{model_name}'...\")\n#     # Create a FastText model\n#     EMBED_SIZE = 300\n#     fast_text_model = FastText(sentences=sentences, vector_size=EMBED_SIZE, window=5, min_count=1, workers=4)\n#     print(f\"Saving '{model_name}'...\")\n#     fast_text_model.save(MODELS_PATH + model_name)\n# else:\n#     print(f\"Loading '{model_name}'...\")\n#     # Load the trained model\n#     fast_text_model = FastText.load(MODELS_PATH + model_name)\n    \n# fast_text_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:06.402685Z","iopub.execute_input":"2024-12-17T23:58:06.403391Z","iopub.status.idle":"2024-12-17T23:58:06.414068Z","shell.execute_reply.started":"2024-12-17T23:58:06.403350Z","shell.execute_reply":"2024-12-17T23:58:06.413089Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df_train = pd.read_parquet('/kaggle/input/dataset-with-is-final-labels/train_95_with_NER_and_IS_labels.parquet') \nprint(df_train.columns)\ndf_train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:06.415326Z","iopub.execute_input":"2024-12-17T23:58:06.415664Z","iopub.status.idle":"2024-12-17T23:58:13.030585Z","shell.execute_reply.started":"2024-12-17T23:58:06.415624Z","shell.execute_reply":"2024-12-17T23:58:13.029717Z"}},"outputs":[{"name":"stdout","text":"Index(['src', 'top', 'tokenized', 'IS_labels', 'NER_labels'], dtype='object')\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(1866898, 5)"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"FEATURE_EXTRACTOR = 'fasttext'\nPIPELINE = 'IS'\nOUTPUT_SIZE = 5 if PIPELINE == 'IS' else 25","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:13.032980Z","iopub.execute_input":"2024-12-17T23:58:13.033374Z","iopub.status.idle":"2024-12-17T23:58:13.037368Z","shell.execute_reply.started":"2024-12-17T23:58:13.033331Z","shell.execute_reply":"2024-12-17T23:58:13.036486Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df_train  = pd.read_parquet('/kaggle/input/dataset-with-is-final-labels/train_95_with_NER_and_IS_labels.parquet')\nprint(df_train.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:13.038825Z","iopub.execute_input":"2024-12-17T23:58:13.039239Z","iopub.status.idle":"2024-12-17T23:58:19.582471Z","shell.execute_reply.started":"2024-12-17T23:58:13.039169Z","shell.execute_reply":"2024-12-17T23:58:19.581466Z"}},"outputs":[{"name":"stdout","text":"                                                                                       src  \\\n0                                            party - size dried peppers pizza and a sprite   \n1    i'd like three pizzas no american cheese and a sprite and five fantas and one perrier   \n2             four seven ups and five 8 ounce coke zeros and three 20 fl ounce pellegrinos   \n3  two 20 fl ounce diet sprites in cans and one ginger ale and one 20 fl oz pineapple soda   \n4                                       a personal size pizza without barbecue pulled pork   \n\n                                                                                                                                                                                                                                               top  \\\n0                                                                                                                             (PIZZAORDER (SIZE party - size ) (TOPPING dried peppers ) pizza ) and (DRINKORDER (NUMBER a ) (DRINKTYPE sprite ) )    \n1  i'd like (PIZZAORDER (NUMBER three ) pizzas no (NOT (TOPPING american cheese ) ) ) and (DRINKORDER (NUMBER a ) (DRINKTYPE sprite ) ) and (DRINKORDER (NUMBER five ) (DRINKTYPE fantas ) ) and (DRINKORDER (NUMBER one ) (DRINKTYPE perrier ) )    \n2                                 (DRINKORDER (NUMBER four ) (DRINKTYPE seven ups ) ) and (DRINKORDER (NUMBER five ) (VOLUME 8 ounce ) (DRINKTYPE coke zeros ) ) and (DRINKORDER (NUMBER three ) (VOLUME 20 fl ounce ) (DRINKTYPE pellegrinos ) )    \n3     (DRINKORDER (NUMBER two ) (VOLUME 20 fl ounce ) (DRINKTYPE diet sprites ) (CONTAINERTYPE in cans ) ) and (DRINKORDER (NUMBER one ) (DRINKTYPE ginger ale ) ) and (DRINKORDER (NUMBER one ) (VOLUME 20 fl oz ) (DRINKTYPE pineapple soda ) )    \n4                                                                                                                                            (PIZZAORDER (NUMBER a ) (SIZE personal size ) pizza without (NOT (TOPPING barbecue pulled pork ) ) )    \n\n                                                                                                     tokenized  \\\n0                                                        [party - size, dried, peppers, pizza, and, a, sprite]   \n1       [i'd, like, three, pizzas, no, american, cheese, and, a, sprite, and, five, fantas, and, one, perrier]   \n2                 [four, seven, ups, and, five, 8, ounce, coke, zeros, and, three, 20, fl, ounce, pellegrinos]   \n3  [two, 20, fl, ounce, diet, sprites, in, cans, and, one, ginger, ale, and, one, 20, fl, oz, pineapple, soda]   \n4                                                  [a, personal, size, pizza, without, barbecue, pulled, pork]   \n\n                                                                              IS_labels  \\\n0                                                  [1, 3, 3, 3, 3, 3, 3, 4, 0, 2, 2, 2]   \n1           [4, 4, 1, 3, 3, 3, 3, 3, 3, 3, 4, 0, 2, 2, 2, 4, 0, 2, 2, 2, 4, 0, 2, 2, 2]   \n2                 [0, 2, 2, 2, 2, 4, 0, 2, 2, 2, 2, 2, 2, 2, 4, 0, 2, 2, 2, 2, 2, 2, 2]   \n3  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 0, 2, 2, 2, 2, 4, 0, 2, 2, 2, 2, 2, 2, 2, 2]   \n4                                                  [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]   \n\n                                                              NER_labels  \n0                                          [8, 20, 10, 22, 24, 24, 6, 3]  \n1               [24, 24, 6, 24, 24, 5, 17, 24, 6, 3, 24, 6, 3, 24, 6, 3]  \n2                 [6, 3, 15, 24, 6, 11, 23, 3, 15, 24, 6, 11, 23, 23, 3]  \n3  [6, 11, 23, 23, 3, 15, 2, 14, 24, 6, 3, 15, 24, 6, 11, 23, 23, 3, 15]  \n4                                          [6, 8, 20, 24, 24, 5, 17, 17]  \n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Read Json data and convert it to parqet format","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n\n# # Load JSON dataset\n# df = pd.read_json('/kaggle/input/nlp-pizzaa-ner-dataset/PIZZA_train.json')\n\n# # Save as Parquet\n# df.to_parquet('/kaggle/working/PIZZA_train_all.parquet', index=False)\n\n# print(\"JSON converted to Parquet!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.583546Z","iopub.execute_input":"2024-12-17T23:58:19.583788Z","iopub.status.idle":"2024-12-17T23:58:19.588257Z","shell.execute_reply.started":"2024-12-17T23:58:19.583764Z","shell.execute_reply":"2024-12-17T23:58:19.587304Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"tqdm.tqdm.pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.589417Z","iopub.execute_input":"2024-12-17T23:58:19.589767Z","iopub.status.idle":"2024-12-17T23:58:19.601986Z","shell.execute_reply.started":"2024-12-17T23:58:19.589729Z","shell.execute_reply":"2024-12-17T23:58:19.601152Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# remove ORDER(","metadata":{}},{"cell_type":"code","source":"# df_train = pd.read_parquet('/kaggle/input/dataset-95-with-ner-and-is-labels/train_95_with_NER_and_IS_labels.parquet')\n# print(df_train['src'].head())\n\n\n\n# df_train['top'] = df_train['top'].str.replace(r\"^\\(ORDER\\s?\", \"\", regex=True)\n# df_train['top'] = df_train['top'].str.replace(r\"\\)$\", \"\", regex=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.603058Z","iopub.execute_input":"2024-12-17T23:58:19.603656Z","iopub.status.idle":"2024-12-17T23:58:19.612834Z","shell.execute_reply.started":"2024-12-17T23:58:19.603617Z","shell.execute_reply":"2024-12-17T23:58:19.611974Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Split Test and Train Data","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.model_selection import train_test_split\n\n# # Load your dataset\n# df = pd.read_parquet('/kaggle/input/train-parquet/train.parquet')\n\n# # Shuffle the dataset\n# df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# # Define features and target\n# X = df['src']  \n# Y = df['top']  \n\n# # Split into train and test sets\n# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.05, random_state=42)\n\n# print(\"Data shuffled and split into training and testing sets.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.613825Z","iopub.execute_input":"2024-12-17T23:58:19.614069Z","iopub.status.idle":"2024-12-17T23:58:19.625212Z","shell.execute_reply.started":"2024-12-17T23:58:19.614045Z","shell.execute_reply":"2024-12-17T23:58:19.624342Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Save Test and Train data","metadata":{}},{"cell_type":"code","source":"# # Combine X_train and y_train into a single DataFrame\n# df_train = pd.DataFrame({'src': X_train, 'top': y_train})\n# df_test = pd.DataFrame({'src': X_test, 'top': y_test})\n\n# # Save to the dataset folder\n# df_train.to_parquet('/kaggle/working/train_95.parquet', index=False)\n# df_test.to_parquet('/kaggle/working/test_5.parquet', index=False)\n\n# print(\"Train and test datasets saved as Parquet files.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.627920Z","iopub.execute_input":"2024-12-17T23:58:19.628184Z","iopub.status.idle":"2024-12-17T23:58:19.637942Z","shell.execute_reply.started":"2024-12-17T23:58:19.628145Z","shell.execute_reply":"2024-12-17T23:58:19.637233Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Tokenize SRC and save it","metadata":{}},{"cell_type":"code","source":"\n# # df_train = pd.read_parquet(\"train.parquet\")\n# token_pattern=r\"(?u)\\b\\w+(?:'\\w+)?(?:\\s*-\\s*\\w+)*\\b\"\n# df_train[\"tokenized\"] = df_train[\"src\"].progress_apply(lambda x: regexp_tokenize(x, token_pattern)) \n# # print(df_train[\"tokenized\"])\n# df_train.to_parquet('/kaggle/working/train_95_with_NER_and_IS_labels.parquet', index=False)\n# print(\"Tokenized src data saved to parquet file.\")\n# print(df_train['tokenized'].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.638807Z","iopub.execute_input":"2024-12-17T23:58:19.639054Z","iopub.status.idle":"2024-12-17T23:58:19.648832Z","shell.execute_reply.started":"2024-12-17T23:58:19.639015Z","shell.execute_reply":"2024-12-17T23:58:19.648012Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Tranform BIO tags to numbers","metadata":{}},{"cell_type":"code","source":"# # # from sklearn.preprocessing import LabelEncoder\n# full_text = \" \".join(df_train['top'].to_list())\n# entities = [x.group() for x in re.finditer(\"(?<=\\()[A-Z]+(_[A-Z]+)*\", full_text)]\n# entities = list(set(entities)) # Unique\n\n#  # Using BIO Tagging\n# bio_entities = [f\"{letter}-{entity}\" for entity in entities for letter in \"BI\"]\n# bio_entities.append('O')\n# bio_entities\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.649734Z","iopub.execute_input":"2024-12-17T23:58:19.649963Z","iopub.status.idle":"2024-12-17T23:58:19.661157Z","shell.execute_reply.started":"2024-12-17T23:58:19.649939Z","shell.execute_reply":"2024-12-17T23:58:19.660362Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"IS_BIO_entities = [\n    'B-PIZZAORDER',\n    'I-PIZZAORDER',\n    'B-DRINKORDER',\n    'I-DRINKORDER',\n    'O'\n] \n\nIS_label_encoder = LabelEncoder()\nIS_label_encoder.fit(IS_BIO_entities)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.662339Z","iopub.execute_input":"2024-12-17T23:58:19.662657Z","iopub.status.idle":"2024-12-17T23:58:19.678387Z","shell.execute_reply.started":"2024-12-17T23:58:19.662620Z","shell.execute_reply":"2024-12-17T23:58:19.677623Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"LabelEncoder()","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"# Extract TOP Target IS and Save it ","metadata":{}},{"cell_type":"code","source":"def extract_labels_IS(top: str, entities):\n    # Extract words and parenthesis\n   \n    token_pattern = r\"\\b\\w+(?:'\\w+)?(?:-\\w+)*\\b|[()]\"\n    # token_pattern=r\"(?u)\\b\\w+(?:'\\w+)?(?:\\s*-\\s*\\w+)*\\b\"\n    tokens = regexp_tokenize(top, token_pattern)\n    labels = []\n    count = 0\n  \n    is_beginning = True\n    order_type = \"PIZZAORDER\"\n    for i, token in enumerate(tokens):\n       \n        if token in entities and token not in [\"PIZZAORDER\", \"DRINKORDER\"]:\n            continue \n      \n        elif token == \"(\":\n            count += 1\n        elif token == \")\":\n            count -= 1\n        elif token == \"PIZZAORDER\":\n            order_type = \"PIZZAORDER\"\n        elif token == \"DRINKORDER\":\n            order_type = \"DRINKORDER\"\n        \n        elif count == 0:\n            labels.append(\"O\")\n            is_beginning = True\n        else:\n            if is_beginning == True:\n                labels.append(\"B-\" + order_type)\n                is_beginning = False\n                continue\n            if is_beginning == False:\n                labels.append(\"I-\" + order_type)\n    labels = IS_label_encoder.transform(labels)\n    return labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.679287Z","iopub.execute_input":"2024-12-17T23:58:19.679552Z","iopub.status.idle":"2024-12-17T23:58:19.685760Z","shell.execute_reply.started":"2024-12-17T23:58:19.679528Z","shell.execute_reply":"2024-12-17T23:58:19.685024Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# # Function to apply extract_labels to each row\n# df_train = pd.read_parquet('/kaggle/input/final-95-train-set/train_95_with_NER_and_IS_labels (1).parquet')\n# def apply_extract_labels(row, entities):\n#     labels = extract_labels_IS(row['top'], entities)\n#     return labels.tolist()\n\n# # # Apply the function to each row in the 'top' column and store the result in a new column 'IS_labels'\n# df_train['IS_labels'] = df_train.progress_apply(lambda row: apply_extract_labels(row,['PIZZAORDER','DRINKORDER'] ), axis=1)\n\n\n# print(df_train[['top', 'IS_labels']].head())\n# labels = df_train['IS_labels']\n# print(IS_label_encoder.inverse_transform(labels[0]))\n\n# # # Save the modified DataFrame to a new Parquet file\n# df_train.to_parquet('/kaggle/working/train_95_with_NER_and_IS_labels.parquet', index=False)\n# print(\"Data with IS_labels saved to train_95\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.686702Z","iopub.execute_input":"2024-12-17T23:58:19.686945Z","iopub.status.idle":"2024-12-17T23:58:19.702437Z","shell.execute_reply.started":"2024-12-17T23:58:19.686921Z","shell.execute_reply":"2024-12-17T23:58:19.701643Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# df_train = pd.read_parquet('/kaggle/input/final-95-train-set/train_95_with_NER_and_IS_labels (1).parquet')\n# # Ensure the values are integers and remove NaN\n# unique_values = df_train['IS_labels'].explode().dropna().unique()\n\n# # # Convert unique_values to integers (if necessary)\n# unique_values = unique_values.astype(int)\n\n# # # Use inverse_transform\n# # print(\"Decoded labels:\", IS_label_encoder.inverse_transform(unique_values))\n\n# # Print unique encoded values\n# print(\"Unique values:\", unique_values) \n# df_train.to_parquet('/kaggle/working/train_95_with_NER_and_IS_labels.parquet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.703479Z","iopub.execute_input":"2024-12-17T23:58:19.703818Z","iopub.status.idle":"2024-12-17T23:58:19.712838Z","shell.execute_reply.started":"2024-12-17T23:58:19.703780Z","shell.execute_reply":"2024-12-17T23:58:19.712120Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# convert parquet to csv","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n\n# # Read the Parquet file\n\n\n# # Save it as a CSV file\n# df.to_csv('/kaggle/working/train_95.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.713701Z","iopub.execute_input":"2024-12-17T23:58:19.713954Z","iopub.status.idle":"2024-12-17T23:58:19.726976Z","shell.execute_reply.started":"2024-12-17T23:58:19.713929Z","shell.execute_reply":"2024-12-17T23:58:19.726226Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# word2vec","metadata":{}},{"cell_type":"code","source":"DATASET_PATH = \"/kaggle/input/pizza-dataset\"\nOUTPUT_ROOT_PATH = \"/kaggle/working\"\nMODELS_PATH = OUTPUT_ROOT_PATH + \"/models\"\nPYTORCH_MODELS_PATH = MODELS_PATH + \"/checkpoints\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.727787Z","iopub.execute_input":"2024-12-17T23:58:19.728095Z","iopub.status.idle":"2024-12-17T23:58:19.738420Z","shell.execute_reply.started":"2024-12-17T23:58:19.728058Z","shell.execute_reply":"2024-12-17T23:58:19.737621Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# os.makedirs(MODELS_PATH, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.739546Z","iopub.execute_input":"2024-12-17T23:58:19.739796Z","iopub.status.idle":"2024-12-17T23:58:19.750599Z","shell.execute_reply.started":"2024-12-17T23:58:19.739772Z","shell.execute_reply":"2024-12-17T23:58:19.749808Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# df_train = pd.read_parquet(\"train.parquet\")\n# print(df_train['tokenized'][0])\n# print(df_train['tokenized'].apply(type).value_counts())\n# print(df_train.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.751525Z","iopub.execute_input":"2024-12-17T23:58:19.751763Z","iopub.status.idle":"2024-12-17T23:58:19.762868Z","shell.execute_reply.started":"2024-12-17T23:58:19.751737Z","shell.execute_reply":"2024-12-17T23:58:19.762080Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# df_train = pd.read_parquet('/kaggle/input/final-95-train-set/train_95_with_NER_and_IS_labels (1).parquet',columns=['tokenized'])\n# sentences=df_train['tokenized'].tolist()\n\n# sentences = [sentence.tolist() if isinstance(sentence, np.ndarray) else sentence for sentence in sentences]\n# '''  \n# <class 'list'>\n# <class 'list'>\n# <class 'str'>\n# ['party', 'size', 'dried', 'peppers', 'pizza', 'and', 'a', 'sprite']\n# '''\n# # Verify the format\n# print(type(sentences))         # Should be list\n# print(type(sentences[0]))      # Should be list\n# print(type(sentences[0][0]))   # Should be str\n# print(sentences[0])            # Check the first sentence\n\n\n\n# # Train Word2Vec model\n# word2vec_model = Word2Vec(sentences=sentences, vector_size=200, window=5, min_count=1, workers=4) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.763779Z","iopub.execute_input":"2024-12-17T23:58:19.764096Z","iopub.status.idle":"2024-12-17T23:58:19.774591Z","shell.execute_reply.started":"2024-12-17T23:58:19.764058Z","shell.execute_reply":"2024-12-17T23:58:19.773772Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# max_length = max(len(sentence) for sentence in df_train['IS_labels']) \n# print(max_length) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.775675Z","iopub.execute_input":"2024-12-17T23:58:19.776027Z","iopub.status.idle":"2024-12-17T23:58:19.783970Z","shell.execute_reply.started":"2024-12-17T23:58:19.775994Z","shell.execute_reply":"2024-12-17T23:58:19.783212Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# import os\n\n# # Define the new directory path\n# new_dir = \"/kaggle/working/feature_extractors\"\n\n# # Create the directory if it doesn't exist\n# if not os.path.exists(new_dir):\n#     os.makedirs(new_dir)\n#     print(f\"Directory created: {new_dir}\")\n# else:\n#     print(\"Directory already exists\")\n\n# word2vec_model.save(\"/kaggle/working/feature_extractors/word2vec_model.model\")\n# print(\"Word2Vec model saved successfully!\")\n# print(word2vec_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.784981Z","iopub.execute_input":"2024-12-17T23:58:19.785249Z","iopub.status.idle":"2024-12-17T23:58:19.793116Z","shell.execute_reply.started":"2024-12-17T23:58:19.785191Z","shell.execute_reply":"2024-12-17T23:58:19.792458Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# import os\n\n# # Specify the file path you want to delete\n# file_path = '/kaggle/working/word2vec_features.npy'  # Replace with the path of the file you want to delete\n\n# # Check if the file exists before deleting it\n# if os.path.exists(file_path):\n#     os.remove(file_path)\n#     print(f\"File {file_path} has been deleted.\")\n# else:\n#     print(f\"File {file_path} not found.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.793979Z","iopub.execute_input":"2024-12-17T23:58:19.794244Z","iopub.status.idle":"2024-12-17T23:58:19.806445Z","shell.execute_reply.started":"2024-12-17T23:58:19.794187Z","shell.execute_reply":"2024-12-17T23:58:19.805652Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"\n\nDATASET_PATH = \"/kaggle/input/pizza-dataset\"\nOUTPUT_ROOT_PATH = \"/kaggle/working\"\nMODELS_PATH = OUTPUT_ROOT_PATH + \"/models\"\nPYTORCH_MODELS_PATH = MODELS_PATH + \"/checkpoints\"\n\n# General\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport re\nimport os\nimport tqdm\n\nimport spacy\n# Load spacy model for lemmatization\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import regexp_tokenize\n\n# Models\n\nimport torch\n\n# Evaluation metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:19.807295Z","iopub.execute_input":"2024-12-17T23:58:19.807509Z","iopub.status.idle":"2024-12-17T23:58:20.449840Z","shell.execute_reply.started":"2024-12-17T23:58:19.807480Z","shell.execute_reply":"2024-12-17T23:58:20.449107Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# # File paths\n# parquet_file_path = '/kaggle/working/train_95.parquet'\n# model_file_path = '/kaggle/input/word2vecmodel/word2vec_model.bin'\n\n# # Load the Parquet file\n# train_data = pd.read_parquet(parquet_file_path)\n# print(\"Parquet file loaded successfully.\")\n\n# # Load the Word2Vec model\n# word2vec_model = Word2Vec.load(model_file_path)\n# print(\"Word2Vec model loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:20.450924Z","iopub.execute_input":"2024-12-17T23:58:20.451215Z","iopub.status.idle":"2024-12-17T23:58:20.455061Z","shell.execute_reply.started":"2024-12-17T23:58:20.451174Z","shell.execute_reply":"2024-12-17T23:58:20.454249Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"\n# Load tokenized data (assuming you have 'tokenized' column in the train.parquet)\n# train_data = pd.read_parquet(\"train.parquet\", columns=[\"tokenized\", \"IS_labels\"])\n# train_data['tokenized'] = train_data['tokenized'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n# Apply the get_word_vectors function to get word vectors for each sentence\n# train_data['word_vectors'] = train_data['tokenized'].apply(lambda x: get_word_vectors(x, word2vec_model)) \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:20.456184Z","iopub.execute_input":"2024-12-17T23:58:20.457167Z","iopub.status.idle":"2024-12-17T23:58:20.466409Z","shell.execute_reply.started":"2024-12-17T23:58:20.457128Z","shell.execute_reply":"2024-12-17T23:58:20.465654Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"markdown","source":"## Define LSTM in pytorch","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.2):\n        super(LSTMModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        # LSTM layer\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n        # Fully connected layer for classification\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        # Pass input through LSTM\n        lstm_out, _ = self.lstm(x)\n        # Use the output of the last time step for classification\n        out = self.fc(lstm_out)  # Shape: (batch_size, output_size)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:20.470527Z","iopub.execute_input":"2024-12-17T23:58:20.470802Z","iopub.status.idle":"2024-12-17T23:58:20.482177Z","shell.execute_reply.started":"2024-12-17T23:58:20.470760Z","shell.execute_reply":"2024-12-17T23:58:20.481493Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"# IS DataSet","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport torch\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import Word2Vec\n\nclass ISDataSet(Dataset):\n    def __init__(self, file, feature_extractor_model, vector_size):\n        # Load the data\n        df_train = pd.read_parquet(file, columns=['tokenized', 'IS_labels'])\n        self.sentences = df_train['tokenized']\n        self.labels = df_train['IS_labels']\n        self.feature_extractor_model = feature_extractor_model  # Word2Vec or FastText model\n        self.vector_size = vector_size  # Size of the word embeddings\n\n    def __len__(self):\n        return len(self.labels)\n       \n\n    def __len__(self):\n        return len(self.labels)\n        \n    def get_sentence_vectors(self, sentence):\n        \"\"\"\n        Convert a tokenized sentence into a list of word vectors using the Word2Vec model.\n        \n        :param sentence: List of tokens.\n        :return: List of word vectors (numpy array).\n        \"\"\"\n        sentence_vectors = []\n        for word in sentence:\n            if word in self.feature_extractor_model.wv.key_to_index:\n                sentence_vectors.append(self.feature_extractor_model.wv[word])  # Word2Vec or  Fasttext\n            else:\n                sentence_vectors.append(np.zeros(self.vector_size))  # Zero vector for unknown words\n        # print(\"Sentence vectors:\", sentence_vectors[:5])  # Displaying a few vectors\n        return sentence_vectors\n\n    def __getitem__(self, idx):\n        # Get the sentence vector\n        feature = self.get_sentence_vectors(self.sentences.iloc[idx])\n        # Apply padding to the sentence vectors\n        padded_feature = pad_sequences([feature], maxlen=50, dtype='float32', padding='post')\n        \n        # For sequence labels, pad them as well\n        label = self.labels.iloc[idx]\n        padded_label = pad_sequences([label], maxlen=50, padding='post', value=-1)  # Padding with -1 for labels\n        return padded_feature[0], padded_label[0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:20.482998Z","iopub.execute_input":"2024-12-17T23:58:20.483261Z","iopub.status.idle":"2024-12-17T23:58:31.186470Z","shell.execute_reply.started":"2024-12-17T23:58:20.483227Z","shell.execute_reply":"2024-12-17T23:58:31.185671Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"# NER Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport torch\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import Word2Vec\n\nclass NERDataSet(Dataset):\n    def __init__(self, file, feature_extractor_model, vector_size):\n        df_train = pd.read_parquet(file,columns=['tokenized','NER_labels'])\n        self.sentences = df_train['tokenized']\n        self.labels = df_train['NER_labels']\n        self.feature_exctractor_model = feature_extractor_model\n        self.vector_size = vector_size\n       \n\n    def __len__(self):\n        return len(self.labels)\n        \n    def get_sentence_vectors(self, sentence):\n        \"\"\"\n        Convert a tokenized sentence into a list of word vectors using the Word2Vec model.\n        \n        :param sentence: List of tokens.\n        :return: List of word vectors (numpy array).\n        \"\"\"\n        sentence_vectors = []\n        for word in sentence:\n            if word in self.feature_exctractor_model.wv.key_to_index:\n                sentence_vectors.append(self.feature_exctractor_model.wv[word])  # Word2Vec vector\n            else:\n                sentence_vectors.append(np.zeros(self.vector_size))  # Zero vector for unknown words\n        # print(\"Sentence vectors:\", sentence_vectors[:5])  # Displaying a few vectors\n        return sentence_vectors\n\n    def __getitem__(self, idx):\n        # Get the sentence vector\n        feature = self.get_sentence_vectors(self.sentences.iloc[idx])\n        # Apply padding to the sentence vectors\n        padded_feature = pad_sequences([feature], maxlen=50, dtype='float32', padding='post')\n        \n        # For sequence labels, pad them as well\n        label = self.labels.iloc[idx]\n        padded_label = pad_sequences([label], maxlen=50, padding='post', value=-1)  # Padding with -1 for labels\n        return padded_feature[0], padded_label[0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:31.187550Z","iopub.execute_input":"2024-12-17T23:58:31.188167Z","iopub.status.idle":"2024-12-17T23:58:31.195963Z","shell.execute_reply.started":"2024-12-17T23:58:31.188135Z","shell.execute_reply":"2024-12-17T23:58:31.195133Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"# Load Feature Extractor","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom gensim.models import FastText\n\nif(FEATURE_EXTRACTOR == 'fasttext'):\n    feature_extractor_model  = FastText.load(\"/kaggle/working/models/fast_text_model.bin\")\n    print(\"fasttext Model loaded successfully!\") \nelse :\n    feature_extractor_model =  Word2Vec.load(\"/kaggle/input/word2vec-model/word2vec_model.model\")\n    print(\"word2vec Model loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:31.197336Z","iopub.execute_input":"2024-12-17T23:58:31.197698Z","iopub.status.idle":"2024-12-17T23:58:32.035224Z","shell.execute_reply.started":"2024-12-17T23:58:31.197660Z","shell.execute_reply":"2024-12-17T23:58:32.034289Z"}},"outputs":[{"name":"stdout","text":"fasttext Model loaded successfully!\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# df_train = pd.read_parquet('/kaggle/input/final-95-train-set/train_95_with_NER_and_IS_labels (1).parquet')\n# # Ensure the values are integers and remove NaN\n# unique_values = df_train['IS_labels'].explode().dropna().unique()\n\n# # Convert unique_values to integers (if necessary)\n# unique_values = unique_values.astype(int)\n\n# # Use inverse_transform\n# print(\"Decoded labels:\", IS_label_encoder.inverse_transform(unique_values))\n\n# # Print unique encoded values\n# print(\"Unique values:\", unique_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:32.036357Z","iopub.execute_input":"2024-12-17T23:58:32.036640Z","iopub.status.idle":"2024-12-17T23:58:32.040828Z","shell.execute_reply.started":"2024-12-17T23:58:32.036613Z","shell.execute_reply":"2024-12-17T23:58:32.039746Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"# Train Loop","metadata":{}},{"cell_type":"code","source":"import torch\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import Word2Vec\n\n# Assuming CustomDataSet and LSTMModel are defined as you provided\n\ndef save_checkpoint(model, optimizer, epoch, loss, file_path):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss\n    }\n    torch.save(checkpoint, file_path)\n    print(f\"Checkpoint saved at {file_path}\")\n\n# Training loop\ndef train_model(file,feature_extractor ,model, num_epochs=10, batch_size=32, learning_rate=0.001, chunk_size=10000,pipeline=\"IS\",feature_extr = \"word2vec\"):\n    # Load the dataset \n    if pipeline == \"IS\":\n        dataset = ISDataSet(file, feature_extractor_model=feature_extractor ,vector_size=feature_extractor.vector_size )\n    else :\n        dataset = NERDataSet(file,feature_extractor_model=feature_extractor ,vector_size=feature_extractor.vector_size )\n        \n    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    # Model, loss function, and optimizer initialization\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss(ignore_index=-1)  # -1 is the padding index\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Training loop\n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        total_correct_train = 0\n        total_samples_train = 0\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n\n        model.train()\n        \n        for batch_idx, (sentences_batch, labels_batch) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}_{pipeline}_{feature_extr}\", unit=\"batch\")):\n            # Move batch data to the appropriate device\n            sentences_batch = sentences_batch.to(device)\n            labels_batch = labels_batch.to(device)\n            # print(\"sentences batch size :\",sentences_batch.shape)\n            # print(\"labels_batch batch size :\",labels_batch.shape)\n            \n           \n            # Forward pass\n            outputs = model(sentences_batch)  # Shape: (batch_size, seq_len, num_classes)\n            \n            # Flatten the output and labels for CrossEntropyLoss\n            outputs_flat = torch.flatten(outputs, start_dim=0, end_dim=1)  # (batch_size * seq_len, num_classes)\n            targets_flat = labels_batch.view(-1).long()  # (batch_size * seq_len)\n            # print(\"output size :\",outputs_flat.shape)\n            # print(\"targets_flat size :\",targets_flat.shape)\n            # Compute loss\n            batch_loss = criterion(outputs_flat, targets_flat)\n            optimizer.zero_grad()\n            batch_loss.backward()\n            optimizer.step()\n\n            # Accumulate loss\n            epoch_loss += batch_loss.item()\n\n            # Accuracy calculation\n            predictions = outputs.argmax(-1)  # (batch_size, seq_len)\n            mask = labels_batch != -1  # Exclude padding from accuracy calculation\n            total_correct_train += (predictions[mask] == labels_batch[mask]).sum().item()\n            total_samples_train += mask.sum().item()  # Count valid labels (non-padding)\n\n        # Calculate average loss and accuracy\n        epoch_loss /= len(train_loader)  # Average loss per batch\n        epoch_acc = total_correct_train / total_samples_train  # Accuracy: correct / total valid labels\n\n        print(f'Epoch [{epoch + 1}/{num_epochs}] | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.4f}')\n        \n        # Save the model after every epoch\n        save_checkpoint(model, optimizer, epoch, epoch_loss, f\"{PIPELINE}_model_{FEATURE_EXTRACTOR}_epoch_{epoch+1}.pth\")\n\n        # Clear GPU cache and collect garbage\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    print(\"Training complete!\")\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:32.042284Z","iopub.execute_input":"2024-12-17T23:58:32.042561Z","iopub.status.idle":"2024-12-17T23:58:32.137119Z","shell.execute_reply.started":"2024-12-17T23:58:32.042536Z","shell.execute_reply":"2024-12-17T23:58:32.136257Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"# NER","metadata":{}},{"cell_type":"code","source":"\n# full_entities = np.load('/kaggle/input/ner-entities/full_entities.npy')\n# # Print the data or its shape\n# print(\"full entities :\")\n# print(full_entities)\n\n\n# # Using BIO Tagging\n# full_bio_entities = [f\"{letter}-{entity}\" for entity in full_entities for letter in \"BI\"]\n# full_bio_entities.append('O')\n# full_bio_entities.append('B-COMPLEX_QUANTITY')\n# full_bio_entities.append('I-COMPLEX_QUANTITY')\n# full_bio_entities.remove('I-PIZZAORDER')\n# full_bio_entities.remove('B-PIZZAORDER')\n# full_bio_entities.remove('B-DRINKORDER')\n# full_bio_entities.remove('I-DRINKORDER')\n# label_encoder = LabelEncoder()\n# label_encoder.fit(full_bio_entities)\n\n# print(full_bio_entities)\n# def extract_NER_labels(top: str, entities):\n#     # Extract words and parenthesis\n#     token_pattern=r\"(?u)\\b\\w+(?:'\\w+)?(?:\\s*-\\s*\\w+)*\\b|[()]\"\n#     tokens = regexp_tokenize(top, token_pattern)\n    \n#     labels = []\n#     count = 0\n#     not_str =\"\"\n#     complex_topping_begin = False \n#     is_beginning = True\n#     order_type = \"\"\n#     for token in tokens:\n#         if token in [\"PIZZAORDER\", \"DRINKORDER\",'ORDER']:\n#             count -= 1\n#             continue\n#         elif token == \"(\":\n#             count += 1\n#         elif token == \")\":\n#             count -= 1\n#             if count == 0:\n#                 is_beginning = True\n#                 complex_topping_begin = False\n#                 not_str = \"\"\n#                 order_type = \"\"\n#             if count < 0:\n#                 count = 0\n#         elif token == \"COMPLEX_TOPPING\":\n#             order_type = \"COMPLEX_TOPPING\"\n#             complex_topping_begin = True\n#         elif token == \"NOT\":\n#             not_str = \"NOT_\"\n#         elif token in entities:\n#             order_type = token\n#         elif count == 0:\n#             labels.append(\"O\")\n           \n#         else:\n#             if complex_topping_begin:\n#                 if is_beginning:\n#                     labels.append(\"B-COMPLEX_\" + order_type)\n#                     is_beginning = False\n#                     continue\n#                 else:\n#                     labels.append(\"I-COMPLEX_\" + order_type)\n#                     continue\n#             if is_beginning:\n#                 labels.append(\"B-\" + not_str + order_type)\n#                 is_beginning = False\n#                 continue\n#             else:\n#                 labels.append(\"I-\" + not_str + order_type) \n#     # print(labels)\n#     labels = label_encoder.transform(labels)\n#     return labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:32.138364Z","iopub.execute_input":"2024-12-17T23:58:32.138637Z","iopub.status.idle":"2024-12-17T23:58:32.153464Z","shell.execute_reply.started":"2024-12-17T23:58:32.138610Z","shell.execute_reply":"2024-12-17T23:58:32.152633Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# # Function to apply extract_labels to each row\n# def apply_extract_ner_labels(row, entities):\n#     # Extract labels for each row's 'top' column\n#     labels = extract_NER_labels(row['top'], entities)\n    \n#     # Return the labels (make sure it's in a proper format for storing in DataFrame)\n#     return labels\n\n# # # Apply the function to each row in the 'top' column and store the result in a new column 'IS_labels'\n# df_train = pd.read_parquet('/kaggle/input/data-95/train_95.parquet')\n# df_train['NER_labels'] = df_train.apply(lambda row: apply_extract_ner_labels(row, full_entities), axis=1)\n# print(df_train.head())\n# # Check the result\n\n\n\n# # Save the modified DataFrame to a new Parquet file\n# #df_train.to_parquet('train_95.parquet', index=False)\n\n# #print(\"Data with IS_labels saved to train_95\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:32.154709Z","iopub.execute_input":"2024-12-17T23:58:32.155095Z","iopub.status.idle":"2024-12-17T23:58:32.167328Z","shell.execute_reply.started":"2024-12-17T23:58:32.155055Z","shell.execute_reply":"2024-12-17T23:58:32.166473Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# print(len(full_bio_entities))\n# df_train.to_parquet('/kaggle/working/train_95_with_ner.parquet', index=False)\n# # ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:32.168154Z","iopub.execute_input":"2024-12-17T23:58:32.168423Z","iopub.status.idle":"2024-12-17T23:58:32.177572Z","shell.execute_reply.started":"2024-12-17T23:58:32.168399Z","shell.execute_reply":"2024-12-17T23:58:32.176932Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# df_train = pd.read_parquet('/kaggle/working/train_95_with_ner.parquet') \n# print(df_train.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:32.178380Z","iopub.execute_input":"2024-12-17T23:58:32.178597Z","iopub.status.idle":"2024-12-17T23:58:32.188418Z","shell.execute_reply.started":"2024-12-17T23:58:32.178575Z","shell.execute_reply":"2024-12-17T23:58:32.187759Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# row_values = df_train[['top', 'NER_labels','src','tokenized']].iloc[0]\n# print(row_values['top'])         # Value from 'top'\n# print(row_values['NER_labels'])  # Value from 'NER_labels'\n# print(row_values['tokenized'])  # Value from 'NER_labels'\n# print(df_train[['top', 'NER_labels']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:32.189214Z","iopub.execute_input":"2024-12-17T23:58:32.189468Z","iopub.status.idle":"2024-12-17T23:58:32.205042Z","shell.execute_reply.started":"2024-12-17T23:58:32.189443Z","shell.execute_reply":"2024-12-17T23:58:32.204244Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"# Training Call","metadata":{}},{"cell_type":"code","source":" train_model(file=\"/kaggle/input/dataset-with-is-final-labels/train_95_with_NER_and_IS_labels.parquet\",\n             feature_extractor=feature_extractor_model ,\n             model=LSTMModel(input_size=feature_extractor_model.vector_size, \n                       hidden_size=256,\n                       output_size=OUTPUT_SIZE,\n                       num_layers=5), \n            feature_extr=FEATURE_EXTRACTOR,\n            pipeline=PIPELINE,\n            num_epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:32.206250Z","iopub.execute_input":"2024-12-17T23:58:32.207006Z","iopub.status.idle":"2024-12-17T23:58:43.140764Z","shell.execute_reply.started":"2024-12-17T23:58:32.206978Z","shell.execute_reply":"2024-12-17T23:58:43.139531Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1_IS_fasttext:   0%|          | 239/58341 [00:06<26:11, 36.98batch/s] \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/dataset-with-is-final-labels/train_95_with_NER_and_IS_labels.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_extractor_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLSTMModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_extractor_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                      \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUTPUT_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m           \u001b[49m\u001b[43mfeature_extr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFEATURE_EXTRACTOR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m           \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPELINE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m           \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[37], line 66\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(file, feature_extractor, model, num_epochs, batch_size, learning_rate, chunk_size, pipeline, feature_extr)\u001b[0m\n\u001b[1;32m     64\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m criterion(outputs_flat, targets_flat)\n\u001b[1;32m     65\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 66\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Accumulate loss\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":43},{"cell_type":"markdown","source":"# Testing IS","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf_dev = pd.read_json('/kaggle/input/dataset-dev/PIZZA_dev.json', lines=True)\nprint(df_dev.head())\n\n# Remove rows where 'dev.PCFG_ERR' is \"true\"\ndf_dev = df_dev[df_dev['dev.PCFG_ERR'] == \"False\"]\n\n# Verify the filtering\nprint(f\"Original rows: {len(df_dev)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:43.141597Z","iopub.status.idle":"2024-12-17T23:58:43.141900Z","shell.execute_reply.started":"2024-12-17T23:58:43.141757Z","shell.execute_reply":"2024-12-17T23:58:43.141772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\ndf_dev['dev.TOP'] = df_dev['dev.TOP'].str.replace(r\"^\\(ORDER\\s?\", \"\", regex=True)\n\n\n\ndf_dev['dev.TOP'] = df_dev['dev.TOP'].str.replace(r\"\\)$\", \"\", regex=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:43.143050Z","iopub.status.idle":"2024-12-17T23:58:43.143518Z","shell.execute_reply.started":"2024-12-17T23:58:43.143273Z","shell.execute_reply":"2024-12-17T23:58:43.143296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to apply extract_labels to each row\ndef apply_extract_labels(row, entities):\n    labels = extract_labels_IS(row['dev.TOP'], entities)\n    return labels.tolist()\n\n# # Apply the function to each row in the 'top' column and store the result in a new column 'IS_labels'\ndf_dev['IS_labels'] = df_dev.progress_apply(lambda row: apply_extract_labels(row,['PIZZAORDER','DRINKORDER'] ), axis=1)\n\n\nprint(df_dev[['dev.TOP', 'IS_labels']].head())\nlabels = df_dev['IS_labels']\nprint(IS_label_encoder.inverse_transform(labels[16]))\n\n# # Save the modified DataFrame to a new Parquet file\ndf_dev.to_parquet('/kaggle/working/dev_95_with_NER_and_IS_labels.parquet', index=False)\nprint(\"Data with IS_labels saved to dev\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:43.145390Z","iopub.status.idle":"2024-12-17T23:58:43.145704Z","shell.execute_reply.started":"2024-12-17T23:58:43.145555Z","shell.execute_reply":"2024-12-17T23:58:43.145571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel_file_path = '/kaggle/input/word2vec-model/word2vec_model.model'\n\n# Load the Word2Vec model\nword2vec_model = Word2Vec.load(model_file_path)\nprint(\"Word2Vec model loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:43.147133Z","iopub.status.idle":"2024-12-17T23:58:43.147468Z","shell.execute_reply.started":"2024-12-17T23:58:43.147320Z","shell.execute_reply":"2024-12-17T23:58:43.147336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to apply extract_labels to each row\ndef apply_extract_ner_labels(row, entities):\n    # Extract labels for each row's 'top' column\n    #print(f\"Processing index: {row.name}\")\n    # print(row['dev.TOP'])\n    labels = extract_NER_labels(row['dev.TOP'], entities)\n    \n    # Return the labels (make sure it's in a proper format for storing in DataFrame)\n    return labels\n\ndf_dev['NER_labels'] = df_dev.apply(lambda row: apply_extract_ner_labels(row, full_entities), axis=1)\nprint(df_dev.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:43.148800Z","iopub.status.idle":"2024-12-17T23:58:43.149079Z","shell.execute_reply.started":"2024-12-17T23:58:43.148945Z","shell.execute_reply":"2024-12-17T23:58:43.148959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"token_pattern=r\"(?u)\\b\\w+(?:'\\w+)?(?:\\s*-\\s*\\w+)*\\b\"\ndf_dev[\"tokenized\"] = df_dev[\"dev.SRC\"].progress_apply(lambda x: regexp_tokenize(x, token_pattern)) \ndf_dev.to_parquet('/kaggle/working/dev_with_labels.parquet', index=False)\nprint(\"Tokenized src data saved to parquet file.\")\nprint(df_dev['tokenized'].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:43.150638Z","iopub.status.idle":"2024-12-17T23:58:43.150935Z","shell.execute_reply.started":"2024-12-17T23:58:43.150795Z","shell.execute_reply":"2024-12-17T23:58:43.150810Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Function to Load Trained Model\ndef load_trained_model(checkpoint_path, model, optimizer=None):\n    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n    model.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    print(f\"Loaded model from {checkpoint_path}\")\n    return model\n\n\n# Evaluate on Dev Dataset\ndef evaluate_model(dev_file, model, word2vec_model, batch_size=32):\n    # Create Dataset and DataLoader\n    dev_dataset = CustomDataSet(dev_file, word2vec_model)\n    dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model.eval()  # Set model to evaluation mode\n\n    all_predictions = []\n    all_labels = []\n\n    with torch.no_grad():\n        for sentences_batch, labels_batch in dev_loader:\n            sentences_batch = sentences_batch.to(device)\n            labels_batch = labels_batch.to(device)\n\n            outputs = model(sentences_batch)\n            predictions = torch.argmax(outputs, dim=-1)  # Get class with highest score\n            \n            # Remove padding for evaluation\n            for i in range(labels_batch.size(0)):  # Batch size\n                valid_labels = labels_batch[i][labels_batch[i] != -1]\n                valid_predictions = predictions[i][:len(valid_labels)]\n                all_labels.extend(valid_labels.cpu().numpy())\n                all_predictions.extend(valid_predictions.cpu().numpy())\n\n    # Calculate Metrics\n    print(\"Classification Report:\")\n    print(classification_report(all_labels, all_predictions))\n    print(\"Accuracy Score:\", accuracy_score(all_labels, all_predictions))\n\n\n# Main Code to Load Model and Evaluate\nif __name__ == \"__main__\":\n    # File paths\n    train_word2vec_file = \"/kaggle/input/word2vec-model/word2vec_model.model\"  # Path to Word2Vec model\n    dev_file = \"/kaggle/input/dataset-dev-parquet/dev_with_labels.parquet\"  # Path to dev dataset\n    checkpoint_path = \"/kaggle/input/is-model/IS_model_epoch_5.pth\"  # Trained model checkpoint\n\n    # Load Word2Vec model\n    word2vec_model = Word2Vec.load(train_word2vec_file)\n\n    # Define model architecture\n    input_size = 200  # Should match Word2Vec vector size\n    hidden_size = 256\n    output_size = 5  # Number of output classes\n    num_layers= 5\n    lstm_model = LSTMModel(input_size, hidden_size, output_size,num_layers)\n\n    # Load trained model\n    lstm_model = load_trained_model(checkpoint_path, lstm_model)\n\n    # Evaluate on dev dataset\n    evaluate_model(dev_file, lstm_model, word2vec_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:58:43.151865Z","iopub.status.idle":"2024-12-17T23:58:43.152171Z","shell.execute_reply.started":"2024-12-17T23:58:43.152024Z","shell.execute_reply":"2024-12-17T23:58:43.152040Z"}},"outputs":[],"execution_count":null}]}