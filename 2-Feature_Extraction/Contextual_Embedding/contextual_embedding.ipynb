{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Hugging Face tokenizers, each model is trained on it's own tokens. \n",
    "[Source](https://discuss.huggingface.co/t/tokenizer-splits-up-pre-split-tokens/2078)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T21:15:23.707922Z",
     "iopub.status.busy": "2024-12-06T21:15:23.707431Z",
     "iopub.status.idle": "2024-12-06T21:15:24.499569Z",
     "shell.execute_reply": "2024-12-06T21:15:24.498371Z",
     "shell.execute_reply.started": "2024-12-06T21:15:23.707878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from 'e:\\\\College\\\\4- Senior 2\\\\Semester 1\\\\NLP\\\\Project\\\\config.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# Add the path to the utils folder\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "import importlib\n",
    "# Custom modules\n",
    "from utils import memory_usage, load_json, process_parquet_in_chunks, file_exists\n",
    "from config import run_config, PROCESSED_DATA_PATH, FEATURES_PATH, MODELS_PATH\n",
    "importlib.reload(sys.modules['utils'])\n",
    "importlib.reload(sys.modules['config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T21:15:24.510775Z",
     "iopub.status.busy": "2024-12-06T21:15:24.510375Z",
     "iopub.status.idle": "2024-12-06T21:15:24.519935Z",
     "shell.execute_reply": "2024-12-06T21:15:24.518949Z",
     "shell.execute_reply.started": "2024-12-06T21:15:24.510724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "run_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(PROCESSED_DATA_PATH + \"/X_train.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['three', 'pizzas', 'no', 'american', 'cheese', 'and', 'a', 'water',\n",
       "       'and', 'one', 'ginger', 'ale', 'and', 'a', 'san', 'pellegrino'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100467,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max(len(seq) for seq in X_train)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jina.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 returns **sentence** embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-small-en', trust_remote_code=True) # trust_remote_code is needed to use the encode method\n",
    "# embeddings = model.encode(['How is the weather today?'.split(), 'What is the current weather like today?'.split()], padding=True, truncation=True, max_length=128, return_tensors='pt', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Some weights of the model checkpoint at jinaai/jina-embeddings-v3 were not used when initializing XLMRobertaModel: ['roberta.emb_ln.bias', 'roberta.emb_ln.weight', 'roberta.embeddings.token_type_embeddings.parametrizations.weight.0.lora_A', 'roberta.embeddings.token_type_embeddings.parametrizations.weight.0.lora_B', 'roberta.embeddings.token_type_embeddings.parametrizations.weight.original', 'roberta.embeddings.word_embeddings.parametrizations.weight.0.lora_A', 'roberta.embeddings.word_embeddings.parametrizations.weight.0.lora_B', 'roberta.embeddings.word_embeddings.parametrizations.weight.original', 'roberta.encoder.layers.0.mixer.Wqkv.bias', 'roberta.encoder.layers.0.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.0.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.0.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.0.mixer.out_proj.bias', 'roberta.encoder.layers.0.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.0.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.0.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.0.mlp.fc1.bias', 'roberta.encoder.layers.0.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.0.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.0.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.0.mlp.fc2.bias', 'roberta.encoder.layers.0.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.0.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.0.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.0.norm1.bias', 'roberta.encoder.layers.0.norm1.weight', 'roberta.encoder.layers.0.norm2.bias', 'roberta.encoder.layers.0.norm2.weight', 'roberta.encoder.layers.1.mixer.Wqkv.bias', 'roberta.encoder.layers.1.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.1.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.1.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.1.mixer.out_proj.bias', 'roberta.encoder.layers.1.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.1.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.1.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.1.mlp.fc1.bias', 'roberta.encoder.layers.1.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.1.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.1.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.1.mlp.fc2.bias', 'roberta.encoder.layers.1.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.1.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.1.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.1.norm1.bias', 'roberta.encoder.layers.1.norm1.weight', 'roberta.encoder.layers.1.norm2.bias', 'roberta.encoder.layers.1.norm2.weight', 'roberta.encoder.layers.10.mixer.Wqkv.bias', 'roberta.encoder.layers.10.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.10.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.10.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.10.mixer.out_proj.bias', 'roberta.encoder.layers.10.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.10.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.10.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.10.mlp.fc1.bias', 'roberta.encoder.layers.10.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.10.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.10.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.10.mlp.fc2.bias', 'roberta.encoder.layers.10.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.10.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.10.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.10.norm1.bias', 'roberta.encoder.layers.10.norm1.weight', 'roberta.encoder.layers.10.norm2.bias', 'roberta.encoder.layers.10.norm2.weight', 'roberta.encoder.layers.11.mixer.Wqkv.bias', 'roberta.encoder.layers.11.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.11.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.11.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.11.mixer.out_proj.bias', 'roberta.encoder.layers.11.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.11.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.11.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.11.mlp.fc1.bias', 'roberta.encoder.layers.11.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.11.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.11.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.11.mlp.fc2.bias', 'roberta.encoder.layers.11.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.11.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.11.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.11.norm1.bias', 'roberta.encoder.layers.11.norm1.weight', 'roberta.encoder.layers.11.norm2.bias', 'roberta.encoder.layers.11.norm2.weight', 'roberta.encoder.layers.12.mixer.Wqkv.bias', 'roberta.encoder.layers.12.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.12.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.12.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.12.mixer.out_proj.bias', 'roberta.encoder.layers.12.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.12.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.12.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.12.mlp.fc1.bias', 'roberta.encoder.layers.12.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.12.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.12.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.12.mlp.fc2.bias', 'roberta.encoder.layers.12.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.12.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.12.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.12.norm1.bias', 'roberta.encoder.layers.12.norm1.weight', 'roberta.encoder.layers.12.norm2.bias', 'roberta.encoder.layers.12.norm2.weight', 'roberta.encoder.layers.13.mixer.Wqkv.bias', 'roberta.encoder.layers.13.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.13.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.13.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.13.mixer.out_proj.bias', 'roberta.encoder.layers.13.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.13.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.13.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.13.mlp.fc1.bias', 'roberta.encoder.layers.13.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.13.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.13.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.13.mlp.fc2.bias', 'roberta.encoder.layers.13.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.13.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.13.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.13.norm1.bias', 'roberta.encoder.layers.13.norm1.weight', 'roberta.encoder.layers.13.norm2.bias', 'roberta.encoder.layers.13.norm2.weight', 'roberta.encoder.layers.14.mixer.Wqkv.bias', 'roberta.encoder.layers.14.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.14.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.14.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.14.mixer.out_proj.bias', 'roberta.encoder.layers.14.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.14.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.14.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.14.mlp.fc1.bias', 'roberta.encoder.layers.14.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.14.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.14.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.14.mlp.fc2.bias', 'roberta.encoder.layers.14.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.14.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.14.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.14.norm1.bias', 'roberta.encoder.layers.14.norm1.weight', 'roberta.encoder.layers.14.norm2.bias', 'roberta.encoder.layers.14.norm2.weight', 'roberta.encoder.layers.15.mixer.Wqkv.bias', 'roberta.encoder.layers.15.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.15.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.15.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.15.mixer.out_proj.bias', 'roberta.encoder.layers.15.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.15.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.15.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.15.mlp.fc1.bias', 'roberta.encoder.layers.15.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.15.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.15.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.15.mlp.fc2.bias', 'roberta.encoder.layers.15.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.15.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.15.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.15.norm1.bias', 'roberta.encoder.layers.15.norm1.weight', 'roberta.encoder.layers.15.norm2.bias', 'roberta.encoder.layers.15.norm2.weight', 'roberta.encoder.layers.16.mixer.Wqkv.bias', 'roberta.encoder.layers.16.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.16.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.16.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.16.mixer.out_proj.bias', 'roberta.encoder.layers.16.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.16.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.16.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.16.mlp.fc1.bias', 'roberta.encoder.layers.16.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.16.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.16.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.16.mlp.fc2.bias', 'roberta.encoder.layers.16.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.16.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.16.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.16.norm1.bias', 'roberta.encoder.layers.16.norm1.weight', 'roberta.encoder.layers.16.norm2.bias', 'roberta.encoder.layers.16.norm2.weight', 'roberta.encoder.layers.17.mixer.Wqkv.bias', 'roberta.encoder.layers.17.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.17.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.17.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.17.mixer.out_proj.bias', 'roberta.encoder.layers.17.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.17.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.17.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.17.mlp.fc1.bias', 'roberta.encoder.layers.17.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.17.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.17.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.17.mlp.fc2.bias', 'roberta.encoder.layers.17.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.17.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.17.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.17.norm1.bias', 'roberta.encoder.layers.17.norm1.weight', 'roberta.encoder.layers.17.norm2.bias', 'roberta.encoder.layers.17.norm2.weight', 'roberta.encoder.layers.18.mixer.Wqkv.bias', 'roberta.encoder.layers.18.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.18.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.18.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.18.mixer.out_proj.bias', 'roberta.encoder.layers.18.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.18.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.18.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.18.mlp.fc1.bias', 'roberta.encoder.layers.18.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.18.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.18.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.18.mlp.fc2.bias', 'roberta.encoder.layers.18.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.18.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.18.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.18.norm1.bias', 'roberta.encoder.layers.18.norm1.weight', 'roberta.encoder.layers.18.norm2.bias', 'roberta.encoder.layers.18.norm2.weight', 'roberta.encoder.layers.19.mixer.Wqkv.bias', 'roberta.encoder.layers.19.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.19.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.19.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.19.mixer.out_proj.bias', 'roberta.encoder.layers.19.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.19.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.19.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.19.mlp.fc1.bias', 'roberta.encoder.layers.19.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.19.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.19.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.19.mlp.fc2.bias', 'roberta.encoder.layers.19.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.19.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.19.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.19.norm1.bias', 'roberta.encoder.layers.19.norm1.weight', 'roberta.encoder.layers.19.norm2.bias', 'roberta.encoder.layers.19.norm2.weight', 'roberta.encoder.layers.2.mixer.Wqkv.bias', 'roberta.encoder.layers.2.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.2.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.2.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.2.mixer.out_proj.bias', 'roberta.encoder.layers.2.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.2.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.2.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.2.mlp.fc1.bias', 'roberta.encoder.layers.2.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.2.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.2.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.2.mlp.fc2.bias', 'roberta.encoder.layers.2.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.2.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.2.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.2.norm1.bias', 'roberta.encoder.layers.2.norm1.weight', 'roberta.encoder.layers.2.norm2.bias', 'roberta.encoder.layers.2.norm2.weight', 'roberta.encoder.layers.20.mixer.Wqkv.bias', 'roberta.encoder.layers.20.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.20.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.20.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.20.mixer.out_proj.bias', 'roberta.encoder.layers.20.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.20.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.20.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.20.mlp.fc1.bias', 'roberta.encoder.layers.20.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.20.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.20.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.20.mlp.fc2.bias', 'roberta.encoder.layers.20.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.20.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.20.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.20.norm1.bias', 'roberta.encoder.layers.20.norm1.weight', 'roberta.encoder.layers.20.norm2.bias', 'roberta.encoder.layers.20.norm2.weight', 'roberta.encoder.layers.21.mixer.Wqkv.bias', 'roberta.encoder.layers.21.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.21.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.21.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.21.mixer.out_proj.bias', 'roberta.encoder.layers.21.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.21.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.21.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.21.mlp.fc1.bias', 'roberta.encoder.layers.21.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.21.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.21.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.21.mlp.fc2.bias', 'roberta.encoder.layers.21.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.21.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.21.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.21.norm1.bias', 'roberta.encoder.layers.21.norm1.weight', 'roberta.encoder.layers.21.norm2.bias', 'roberta.encoder.layers.21.norm2.weight', 'roberta.encoder.layers.22.mixer.Wqkv.bias', 'roberta.encoder.layers.22.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.22.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.22.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.22.mixer.out_proj.bias', 'roberta.encoder.layers.22.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.22.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.22.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.22.mlp.fc1.bias', 'roberta.encoder.layers.22.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.22.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.22.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.22.mlp.fc2.bias', 'roberta.encoder.layers.22.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.22.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.22.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.22.norm1.bias', 'roberta.encoder.layers.22.norm1.weight', 'roberta.encoder.layers.22.norm2.bias', 'roberta.encoder.layers.22.norm2.weight', 'roberta.encoder.layers.23.mixer.Wqkv.bias', 'roberta.encoder.layers.23.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.23.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.23.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.23.mixer.out_proj.bias', 'roberta.encoder.layers.23.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.23.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.23.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.23.mlp.fc1.bias', 'roberta.encoder.layers.23.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.23.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.23.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.23.mlp.fc2.bias', 'roberta.encoder.layers.23.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.23.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.23.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.23.norm1.bias', 'roberta.encoder.layers.23.norm1.weight', 'roberta.encoder.layers.23.norm2.bias', 'roberta.encoder.layers.23.norm2.weight', 'roberta.encoder.layers.3.mixer.Wqkv.bias', 'roberta.encoder.layers.3.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.3.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.3.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.3.mixer.out_proj.bias', 'roberta.encoder.layers.3.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.3.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.3.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.3.mlp.fc1.bias', 'roberta.encoder.layers.3.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.3.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.3.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.3.mlp.fc2.bias', 'roberta.encoder.layers.3.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.3.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.3.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.3.norm1.bias', 'roberta.encoder.layers.3.norm1.weight', 'roberta.encoder.layers.3.norm2.bias', 'roberta.encoder.layers.3.norm2.weight', 'roberta.encoder.layers.4.mixer.Wqkv.bias', 'roberta.encoder.layers.4.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.4.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.4.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.4.mixer.out_proj.bias', 'roberta.encoder.layers.4.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.4.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.4.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.4.mlp.fc1.bias', 'roberta.encoder.layers.4.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.4.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.4.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.4.mlp.fc2.bias', 'roberta.encoder.layers.4.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.4.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.4.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.4.norm1.bias', 'roberta.encoder.layers.4.norm1.weight', 'roberta.encoder.layers.4.norm2.bias', 'roberta.encoder.layers.4.norm2.weight', 'roberta.encoder.layers.5.mixer.Wqkv.bias', 'roberta.encoder.layers.5.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.5.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.5.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.5.mixer.out_proj.bias', 'roberta.encoder.layers.5.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.5.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.5.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.5.mlp.fc1.bias', 'roberta.encoder.layers.5.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.5.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.5.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.5.mlp.fc2.bias', 'roberta.encoder.layers.5.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.5.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.5.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.5.norm1.bias', 'roberta.encoder.layers.5.norm1.weight', 'roberta.encoder.layers.5.norm2.bias', 'roberta.encoder.layers.5.norm2.weight', 'roberta.encoder.layers.6.mixer.Wqkv.bias', 'roberta.encoder.layers.6.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.6.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.6.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.6.mixer.out_proj.bias', 'roberta.encoder.layers.6.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.6.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.6.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.6.mlp.fc1.bias', 'roberta.encoder.layers.6.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.6.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.6.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.6.mlp.fc2.bias', 'roberta.encoder.layers.6.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.6.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.6.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.6.norm1.bias', 'roberta.encoder.layers.6.norm1.weight', 'roberta.encoder.layers.6.norm2.bias', 'roberta.encoder.layers.6.norm2.weight', 'roberta.encoder.layers.7.mixer.Wqkv.bias', 'roberta.encoder.layers.7.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.7.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.7.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.7.mixer.out_proj.bias', 'roberta.encoder.layers.7.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.7.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.7.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.7.mlp.fc1.bias', 'roberta.encoder.layers.7.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.7.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.7.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.7.mlp.fc2.bias', 'roberta.encoder.layers.7.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.7.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.7.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.7.norm1.bias', 'roberta.encoder.layers.7.norm1.weight', 'roberta.encoder.layers.7.norm2.bias', 'roberta.encoder.layers.7.norm2.weight', 'roberta.encoder.layers.8.mixer.Wqkv.bias', 'roberta.encoder.layers.8.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.8.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.8.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.8.mixer.out_proj.bias', 'roberta.encoder.layers.8.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.8.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.8.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.8.mlp.fc1.bias', 'roberta.encoder.layers.8.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.8.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.8.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.8.mlp.fc2.bias', 'roberta.encoder.layers.8.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.8.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.8.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.8.norm1.bias', 'roberta.encoder.layers.8.norm1.weight', 'roberta.encoder.layers.8.norm2.bias', 'roberta.encoder.layers.8.norm2.weight', 'roberta.encoder.layers.9.mixer.Wqkv.bias', 'roberta.encoder.layers.9.mixer.Wqkv.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.9.mixer.Wqkv.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.9.mixer.Wqkv.parametrizations.weight.original', 'roberta.encoder.layers.9.mixer.out_proj.bias', 'roberta.encoder.layers.9.mixer.out_proj.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.9.mixer.out_proj.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.9.mixer.out_proj.parametrizations.weight.original', 'roberta.encoder.layers.9.mlp.fc1.bias', 'roberta.encoder.layers.9.mlp.fc1.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.9.mlp.fc1.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.9.mlp.fc1.parametrizations.weight.original', 'roberta.encoder.layers.9.mlp.fc2.bias', 'roberta.encoder.layers.9.mlp.fc2.parametrizations.weight.0.lora_A', 'roberta.encoder.layers.9.mlp.fc2.parametrizations.weight.0.lora_B', 'roberta.encoder.layers.9.mlp.fc2.parametrizations.weight.original', 'roberta.encoder.layers.9.norm1.bias', 'roberta.encoder.layers.9.norm1.weight', 'roberta.encoder.layers.9.norm2.bias', 'roberta.encoder.layers.9.norm2.weight', 'roberta.pooler.dense.parametrizations.weight.0.lora_A', 'roberta.pooler.dense.parametrizations.weight.0.lora_B', 'roberta.pooler.dense.parametrizations.weight.original']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v3 and are newly initialized: ['roberta.embeddings.LayerNorm.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.12.attention.output.LayerNorm.bias', 'roberta.encoder.layer.12.attention.output.LayerNorm.weight', 'roberta.encoder.layer.12.attention.output.dense.bias', 'roberta.encoder.layer.12.attention.output.dense.weight', 'roberta.encoder.layer.12.attention.self.key.bias', 'roberta.encoder.layer.12.attention.self.key.weight', 'roberta.encoder.layer.12.attention.self.query.bias', 'roberta.encoder.layer.12.attention.self.query.weight', 'roberta.encoder.layer.12.attention.self.value.bias', 'roberta.encoder.layer.12.attention.self.value.weight', 'roberta.encoder.layer.12.intermediate.dense.bias', 'roberta.encoder.layer.12.intermediate.dense.weight', 'roberta.encoder.layer.12.output.LayerNorm.bias', 'roberta.encoder.layer.12.output.LayerNorm.weight', 'roberta.encoder.layer.12.output.dense.bias', 'roberta.encoder.layer.12.output.dense.weight', 'roberta.encoder.layer.13.attention.output.LayerNorm.bias', 'roberta.encoder.layer.13.attention.output.LayerNorm.weight', 'roberta.encoder.layer.13.attention.output.dense.bias', 'roberta.encoder.layer.13.attention.output.dense.weight', 'roberta.encoder.layer.13.attention.self.key.bias', 'roberta.encoder.layer.13.attention.self.key.weight', 'roberta.encoder.layer.13.attention.self.query.bias', 'roberta.encoder.layer.13.attention.self.query.weight', 'roberta.encoder.layer.13.attention.self.value.bias', 'roberta.encoder.layer.13.attention.self.value.weight', 'roberta.encoder.layer.13.intermediate.dense.bias', 'roberta.encoder.layer.13.intermediate.dense.weight', 'roberta.encoder.layer.13.output.LayerNorm.bias', 'roberta.encoder.layer.13.output.LayerNorm.weight', 'roberta.encoder.layer.13.output.dense.bias', 'roberta.encoder.layer.13.output.dense.weight', 'roberta.encoder.layer.14.attention.output.LayerNorm.bias', 'roberta.encoder.layer.14.attention.output.LayerNorm.weight', 'roberta.encoder.layer.14.attention.output.dense.bias', 'roberta.encoder.layer.14.attention.output.dense.weight', 'roberta.encoder.layer.14.attention.self.key.bias', 'roberta.encoder.layer.14.attention.self.key.weight', 'roberta.encoder.layer.14.attention.self.query.bias', 'roberta.encoder.layer.14.attention.self.query.weight', 'roberta.encoder.layer.14.attention.self.value.bias', 'roberta.encoder.layer.14.attention.self.value.weight', 'roberta.encoder.layer.14.intermediate.dense.bias', 'roberta.encoder.layer.14.intermediate.dense.weight', 'roberta.encoder.layer.14.output.LayerNorm.bias', 'roberta.encoder.layer.14.output.LayerNorm.weight', 'roberta.encoder.layer.14.output.dense.bias', 'roberta.encoder.layer.14.output.dense.weight', 'roberta.encoder.layer.15.attention.output.LayerNorm.bias', 'roberta.encoder.layer.15.attention.output.LayerNorm.weight', 'roberta.encoder.layer.15.attention.output.dense.bias', 'roberta.encoder.layer.15.attention.output.dense.weight', 'roberta.encoder.layer.15.attention.self.key.bias', 'roberta.encoder.layer.15.attention.self.key.weight', 'roberta.encoder.layer.15.attention.self.query.bias', 'roberta.encoder.layer.15.attention.self.query.weight', 'roberta.encoder.layer.15.attention.self.value.bias', 'roberta.encoder.layer.15.attention.self.value.weight', 'roberta.encoder.layer.15.intermediate.dense.bias', 'roberta.encoder.layer.15.intermediate.dense.weight', 'roberta.encoder.layer.15.output.LayerNorm.bias', 'roberta.encoder.layer.15.output.LayerNorm.weight', 'roberta.encoder.layer.15.output.dense.bias', 'roberta.encoder.layer.15.output.dense.weight', 'roberta.encoder.layer.16.attention.output.LayerNorm.bias', 'roberta.encoder.layer.16.attention.output.LayerNorm.weight', 'roberta.encoder.layer.16.attention.output.dense.bias', 'roberta.encoder.layer.16.attention.output.dense.weight', 'roberta.encoder.layer.16.attention.self.key.bias', 'roberta.encoder.layer.16.attention.self.key.weight', 'roberta.encoder.layer.16.attention.self.query.bias', 'roberta.encoder.layer.16.attention.self.query.weight', 'roberta.encoder.layer.16.attention.self.value.bias', 'roberta.encoder.layer.16.attention.self.value.weight', 'roberta.encoder.layer.16.intermediate.dense.bias', 'roberta.encoder.layer.16.intermediate.dense.weight', 'roberta.encoder.layer.16.output.LayerNorm.bias', 'roberta.encoder.layer.16.output.LayerNorm.weight', 'roberta.encoder.layer.16.output.dense.bias', 'roberta.encoder.layer.16.output.dense.weight', 'roberta.encoder.layer.17.attention.output.LayerNorm.bias', 'roberta.encoder.layer.17.attention.output.LayerNorm.weight', 'roberta.encoder.layer.17.attention.output.dense.bias', 'roberta.encoder.layer.17.attention.output.dense.weight', 'roberta.encoder.layer.17.attention.self.key.bias', 'roberta.encoder.layer.17.attention.self.key.weight', 'roberta.encoder.layer.17.attention.self.query.bias', 'roberta.encoder.layer.17.attention.self.query.weight', 'roberta.encoder.layer.17.attention.self.value.bias', 'roberta.encoder.layer.17.attention.self.value.weight', 'roberta.encoder.layer.17.intermediate.dense.bias', 'roberta.encoder.layer.17.intermediate.dense.weight', 'roberta.encoder.layer.17.output.LayerNorm.bias', 'roberta.encoder.layer.17.output.LayerNorm.weight', 'roberta.encoder.layer.17.output.dense.bias', 'roberta.encoder.layer.17.output.dense.weight', 'roberta.encoder.layer.18.attention.output.LayerNorm.bias', 'roberta.encoder.layer.18.attention.output.LayerNorm.weight', 'roberta.encoder.layer.18.attention.output.dense.bias', 'roberta.encoder.layer.18.attention.output.dense.weight', 'roberta.encoder.layer.18.attention.self.key.bias', 'roberta.encoder.layer.18.attention.self.key.weight', 'roberta.encoder.layer.18.attention.self.query.bias', 'roberta.encoder.layer.18.attention.self.query.weight', 'roberta.encoder.layer.18.attention.self.value.bias', 'roberta.encoder.layer.18.attention.self.value.weight', 'roberta.encoder.layer.18.intermediate.dense.bias', 'roberta.encoder.layer.18.intermediate.dense.weight', 'roberta.encoder.layer.18.output.LayerNorm.bias', 'roberta.encoder.layer.18.output.LayerNorm.weight', 'roberta.encoder.layer.18.output.dense.bias', 'roberta.encoder.layer.18.output.dense.weight', 'roberta.encoder.layer.19.attention.output.LayerNorm.bias', 'roberta.encoder.layer.19.attention.output.LayerNorm.weight', 'roberta.encoder.layer.19.attention.output.dense.bias', 'roberta.encoder.layer.19.attention.output.dense.weight', 'roberta.encoder.layer.19.attention.self.key.bias', 'roberta.encoder.layer.19.attention.self.key.weight', 'roberta.encoder.layer.19.attention.self.query.bias', 'roberta.encoder.layer.19.attention.self.query.weight', 'roberta.encoder.layer.19.attention.self.value.bias', 'roberta.encoder.layer.19.attention.self.value.weight', 'roberta.encoder.layer.19.intermediate.dense.bias', 'roberta.encoder.layer.19.intermediate.dense.weight', 'roberta.encoder.layer.19.output.LayerNorm.bias', 'roberta.encoder.layer.19.output.LayerNorm.weight', 'roberta.encoder.layer.19.output.dense.bias', 'roberta.encoder.layer.19.output.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.20.attention.output.LayerNorm.bias', 'roberta.encoder.layer.20.attention.output.LayerNorm.weight', 'roberta.encoder.layer.20.attention.output.dense.bias', 'roberta.encoder.layer.20.attention.output.dense.weight', 'roberta.encoder.layer.20.attention.self.key.bias', 'roberta.encoder.layer.20.attention.self.key.weight', 'roberta.encoder.layer.20.attention.self.query.bias', 'roberta.encoder.layer.20.attention.self.query.weight', 'roberta.encoder.layer.20.attention.self.value.bias', 'roberta.encoder.layer.20.attention.self.value.weight', 'roberta.encoder.layer.20.intermediate.dense.bias', 'roberta.encoder.layer.20.intermediate.dense.weight', 'roberta.encoder.layer.20.output.LayerNorm.bias', 'roberta.encoder.layer.20.output.LayerNorm.weight', 'roberta.encoder.layer.20.output.dense.bias', 'roberta.encoder.layer.20.output.dense.weight', 'roberta.encoder.layer.21.attention.output.LayerNorm.bias', 'roberta.encoder.layer.21.attention.output.LayerNorm.weight', 'roberta.encoder.layer.21.attention.output.dense.bias', 'roberta.encoder.layer.21.attention.output.dense.weight', 'roberta.encoder.layer.21.attention.self.key.bias', 'roberta.encoder.layer.21.attention.self.key.weight', 'roberta.encoder.layer.21.attention.self.query.bias', 'roberta.encoder.layer.21.attention.self.query.weight', 'roberta.encoder.layer.21.attention.self.value.bias', 'roberta.encoder.layer.21.attention.self.value.weight', 'roberta.encoder.layer.21.intermediate.dense.bias', 'roberta.encoder.layer.21.intermediate.dense.weight', 'roberta.encoder.layer.21.output.LayerNorm.bias', 'roberta.encoder.layer.21.output.LayerNorm.weight', 'roberta.encoder.layer.21.output.dense.bias', 'roberta.encoder.layer.21.output.dense.weight', 'roberta.encoder.layer.22.attention.output.LayerNorm.bias', 'roberta.encoder.layer.22.attention.output.LayerNorm.weight', 'roberta.encoder.layer.22.attention.output.dense.bias', 'roberta.encoder.layer.22.attention.output.dense.weight', 'roberta.encoder.layer.22.attention.self.key.bias', 'roberta.encoder.layer.22.attention.self.key.weight', 'roberta.encoder.layer.22.attention.self.query.bias', 'roberta.encoder.layer.22.attention.self.query.weight', 'roberta.encoder.layer.22.attention.self.value.bias', 'roberta.encoder.layer.22.attention.self.value.weight', 'roberta.encoder.layer.22.intermediate.dense.bias', 'roberta.encoder.layer.22.intermediate.dense.weight', 'roberta.encoder.layer.22.output.LayerNorm.bias', 'roberta.encoder.layer.22.output.LayerNorm.weight', 'roberta.encoder.layer.22.output.dense.bias', 'roberta.encoder.layer.22.output.dense.weight', 'roberta.encoder.layer.23.attention.output.LayerNorm.bias', 'roberta.encoder.layer.23.attention.output.LayerNorm.weight', 'roberta.encoder.layer.23.attention.output.dense.bias', 'roberta.encoder.layer.23.attention.output.dense.weight', 'roberta.encoder.layer.23.attention.self.key.bias', 'roberta.encoder.layer.23.attention.self.key.weight', 'roberta.encoder.layer.23.attention.self.query.bias', 'roberta.encoder.layer.23.attention.self.query.weight', 'roberta.encoder.layer.23.attention.self.value.bias', 'roberta.encoder.layer.23.attention.self.value.weight', 'roberta.encoder.layer.23.intermediate.dense.bias', 'roberta.encoder.layer.23.intermediate.dense.weight', 'roberta.encoder.layer.23.output.LayerNorm.bias', 'roberta.encoder.layer.23.output.LayerNorm.weight', 'roberta.encoder.layer.23.output.dense.bias', 'roberta.encoder.layer.23.output.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "feature_extraction = pipeline('feature-extraction', model=\"jinaai/jina-embeddings-v3\", tokenizer=\"jinaai/jina-embeddings-v3\", device=device, trust_remote_code=True)\n",
    "features = feature_extraction([\"I am a longer sentence more than 5 tokens\", \"I am a short sentence\"], padding=True, truncation=True, max_length=128, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load RoBERTa model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RoBERTa: It follows the same approach as BERT for input representations but uses **byte pair encoding** (BPE) with a larger vocabulary size (up to 50,000 tokens). [Source](https://dsstream.com/roberta-vs-bert-exploring-the-evolution-of-transformer-models/#:~:text=RoBERTa%3A%20It%20follows%20the%20same,Next%20Sentence%20Prediction%20(NSP))\n",
    "- [Best Embeddings Models](https://www.reddit.com/r/LocalLLaMA/comments/18j39qt/what_embedding_models_are_you_using_for_rag/)\n",
    "- [Pipeline Reference](https://huggingface.co/docs/transformers/main_classes/pipelines)\n",
    "- BPE will help overcome spelling mistakes (in my opinion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T21:16:59.497134Z",
     "iopub.status.busy": "2024-12-06T21:16:59.496761Z",
     "iopub.status.idle": "2024-12-06T21:16:59.516975Z",
     "shell.execute_reply": "2024-12-06T21:16:59.515691Z",
     "shell.execute_reply.started": "2024-12-06T21:16:59.497101Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "update_feature = False\n",
    "if update_feature or not file_exists(MODELS_PATH + \"/contextual_embeddings.parquet\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n",
    "    pipe = pipeline(\"feature-extraction\", model=\"jinaai/jina-embeddings-v3\", trust_remote_code=True, device=device)\n",
    "    pipe.model.eval()\n",
    "\n",
    "    # Convert your data to a Hugging Face dataset\n",
    "    dataset = Dataset.from_pandas(pd.DataFrame(X_train, columns=[\"tokenized\"]))\n",
    "\n",
    "    # Use KeyDataset to create a dataset that extracts the \"tokenized\" field\n",
    "    key_dataset = KeyDataset(dataset, \"tokenized\")\n",
    "\n",
    "    # Initialize an empty list to store embeddings\n",
    "    all_embeddings = []\n",
    "\n",
    "    # # Process the dataset in batches and collect embeddings\n",
    "    # for out in pipe(key_dataset, batch_size=8, padding='max_length', truncation=True, max_length=max_length):\n",
    "    #     all_embeddings.extend(out)\n",
    "    # Process the dataset in batches and collect embeddings\n",
    "    for batch in key_dataset:\n",
    "        tokenized_inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        print(tokenized_inputs)\n",
    "        embeddings = pipe(tokenized_inputs[\"input_ids\"])\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "\n",
    "    # Convert the embeddings to a DataFrame\n",
    "    embeddings = pd.DataFrame(all_embeddings)\n",
    "\n",
    "    # Save the embeddings to a parquet file\n",
    "    embeddings.to_parquet(MODELS_PATH + \"/contextual_embeddings.parquet\")\n",
    "else:\n",
    "    print(\"Loading the model\")\n",
    "    embeddings = pd.read_parquet(MODELS_PATH + \"/contextual_embeddings.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2093, 10733,  2015,  2053,  2137,  8808,  1998,  1037,  2300,\n",
      "          1998,  2028, 14580, 15669,  1998,  1037,  2624, 21877,  6216, 24860,\n",
      "          2080,   102]])\n",
      "[CLS] three pizzas no american cheese and a water and one ginger ale and a san pellegrino [SEP]\n",
      "['[CLS]', 'three', 'pizza', '##s', 'no', 'american', 'cheese', 'and', 'a', 'water', 'and', 'one', 'ginger', 'ale', 'and', 'a', 'san', 'pe', '##lle', '##grin', '##o', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"This is an example sentence\", \"Each sentence is converted word unqiue\"]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(list(X_train[0]), padding=True, truncation=True, return_tensors='pt', is_split_into_words=True, )\n",
    "\n",
    "print(encoded_input.input_ids)\n",
    "\n",
    "print(tokenizer.decode(encoded_input.input_ids[0]))\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(encoded_input.input_ids[0]))\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 384])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related Resources:\n",
    "- TF-IDF Matrix -> https://openclassrooms.com/en/courses/6532301-introduction-to-natural-language-processing/8081363-apply-the-tf-idf-vectorization-approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T21:18:33.418337Z",
     "iopub.status.busy": "2024-12-06T21:18:33.417911Z",
     "iopub.status.idle": "2024-12-06T21:18:34.019152Z",
     "shell.execute_reply": "2024-12-06T21:18:34.017875Z",
     "shell.execute_reply.started": "2024-12-06T21:18:33.418302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def vectorize_words(row):\n",
    "    tokens: list[str] = row[\"tokenized\"]\n",
    "    sentence_tfidfs = []\n",
    "    sentence_word2vec = []\n",
    "    for token in tokens:\n",
    "        tfidf_index = vectorizer.vocabulary_.get(token, 0) # Default index zero\n",
    "        sentence_tfidfs.append(tfidf_features[:, tfidf_index].toarray().reshape(-1))\n",
    "        sentence_word2vec.append(word_embeddings.get(token, [0] * 100)) # Default zero-vector\n",
    "    row['tfidf_features'], row['word2vec_features'] = sentence_tfidfs, sentence_word2vec\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 58/2456446 [01:25<934:18:24,  1.37s/it] "
     ]
    }
   ],
   "source": [
    "df_train = df_train.progress_apply(vectorize_words, axis=1)\n",
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 348/348 [00:00<00:00, 499.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>top</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tfidf_features</th>\n",
       "      <th>word2vec_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i want to order two medium pizzas with sausage and black olives and two medium pizzas with pepperoni and extra cheese and three large pizzas with pepperoni and sausage</td>\n",
       "      <td>i want to order (PIZZAORDER (NUMBER two ) (SIZE medium ) pizzas with (TOPPING sausage ) and (TOPPING black olives ) ) and (PIZZAORDER (NUMBER two ) (SIZE medium ) pizzas with (TOPPING pepperoni ) and (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING cheese ) ) ) and (PIZZAORDER (NUMBER three ) (SIZE large ) pizzas with (TOPPING pepperoni ) and (TOPPING sausage ) )</td>\n",
       "      <td>[i, want, to, order, two, medium, pizzas, with, sausage, and, black, olives, and, two, medium, pizzas, with, pepperoni, and, extra, cheese, and, three, large, pizzas, with, pepperoni, and, sausage]</td>\n",
       "      <td>[[0.08430143723922251, 0.0, 0.13337969831862775, 0.0, 0.0, 0.10034713760427152, 0.17396375337864922, 0.17958770538428326, 0.14606531916873786, 0.17721922092796344, 0.19713448714781656, 0.0, 0.17288347347903418, 0.0, 0.0, 0.0, 0.1366724335010161, 0.0, 0.0, 0.1562794386914836, 0.19391225920436742, 0.0, 0.11286164475205294, 0.0, 0.12416543512769322, 0.0, 0.0, 0.19234760746279847, 0.12248119173361699, 0.0, 0.20664361011193244, 0.0, 0.18995951017233198, 0.2045777012433944, 0.1168537902697436, 0.0, 0.16890313266495663, 0.0, 0.0, 0.1335421575549344, 0.0, 0.19944260753166945, 0.0, 0.18473089869790738, 0.0, 0.19133428377261583, 0.20978968793267175, 0.14310616380645452, 0.17265659900612262, 0.0, 0.16817054006660384, 0.0, 0.14150333339868498, 0.17275472419101204, 0.22930989128613138, 0.0, 0.0, 0.20155267717081113, 0.1546082459604938, 0.15693159662581818, 0.1832160787506764, 0.0, 0.18729363892005363, 0.0, 0.20773045712657492, 0.0, 0.19346051503038109, 0.0, 0.16041910398343542, 0.09303061188182...</td>\n",
       "      <td>[[0.0677469, -0.041739315, -0.08083352, 0.12261514, 0.16721858, -0.05736017, 0.09708657, 0.28409472, -0.118705854, -0.027471012, -0.057421274, -0.14376749, 0.0473368, 0.14283921, -0.030394064, -0.031858526, 0.052781112, 0.07924095, 0.0069334647, -0.2780961, 0.09244968, -0.032641098, -0.026583068, 0.036451172, 0.03341323, -0.025228117, -0.07656543, -0.118221, -0.10734222, 0.03260397, 0.15275688, 0.05852104, 0.106173486, -0.05881508, -0.082549885, 0.1174997, 0.06174134, -0.08423522, -0.055344716, -0.16970553, -0.06645013, 0.006280372, -0.11624798, -0.00490635, 0.1332938, -0.05824645, -0.00758351, -0.1255734, 0.039873965, 0.07710664, 0.045426518, -0.18234527, -0.024939088, -0.10234514, 0.08368705, -0.06852132, -0.07534842, -0.15418817, -0.14619642, -0.003937893, -0.101910375, -0.030932281, 0.1487361, -0.090863645, -0.320359, 0.09530128, -0.022416687, 0.22749043, -0.17629756, 0.21058744, -0.03136628, 0.1173821, 0.12045498, 0.03843337, 0.11156459, 0.058297314, 0.104958236, -0.11261302, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                       src  \\\n",
       "0  i want to order two medium pizzas with sausage and black olives and two medium pizzas with pepperoni and extra cheese and three large pizzas with pepperoni and sausage   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                         top  \\\n",
       "0  i want to order (PIZZAORDER (NUMBER two ) (SIZE medium ) pizzas with (TOPPING sausage ) and (TOPPING black olives ) ) and (PIZZAORDER (NUMBER two ) (SIZE medium ) pizzas with (TOPPING pepperoni ) and (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING cheese ) ) ) and (PIZZAORDER (NUMBER three ) (SIZE large ) pizzas with (TOPPING pepperoni ) and (TOPPING sausage ) )    \n",
       "\n",
       "                                                                                                                                                                                               tokenized  \\\n",
       "0  [i, want, to, order, two, medium, pizzas, with, sausage, and, black, olives, and, two, medium, pizzas, with, pepperoni, and, extra, cheese, and, three, large, pizzas, with, pepperoni, and, sausage]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            tfidf_features  \\\n",
       "0  [[0.08430143723922251, 0.0, 0.13337969831862775, 0.0, 0.0, 0.10034713760427152, 0.17396375337864922, 0.17958770538428326, 0.14606531916873786, 0.17721922092796344, 0.19713448714781656, 0.0, 0.17288347347903418, 0.0, 0.0, 0.0, 0.1366724335010161, 0.0, 0.0, 0.1562794386914836, 0.19391225920436742, 0.0, 0.11286164475205294, 0.0, 0.12416543512769322, 0.0, 0.0, 0.19234760746279847, 0.12248119173361699, 0.0, 0.20664361011193244, 0.0, 0.18995951017233198, 0.2045777012433944, 0.1168537902697436, 0.0, 0.16890313266495663, 0.0, 0.0, 0.1335421575549344, 0.0, 0.19944260753166945, 0.0, 0.18473089869790738, 0.0, 0.19133428377261583, 0.20978968793267175, 0.14310616380645452, 0.17265659900612262, 0.0, 0.16817054006660384, 0.0, 0.14150333339868498, 0.17275472419101204, 0.22930989128613138, 0.0, 0.0, 0.20155267717081113, 0.1546082459604938, 0.15693159662581818, 0.1832160787506764, 0.0, 0.18729363892005363, 0.0, 0.20773045712657492, 0.0, 0.19346051503038109, 0.0, 0.16041910398343542, 0.09303061188182...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         word2vec_features  \n",
       "0  [[0.0677469, -0.041739315, -0.08083352, 0.12261514, 0.16721858, -0.05736017, 0.09708657, 0.28409472, -0.118705854, -0.027471012, -0.057421274, -0.14376749, 0.0473368, 0.14283921, -0.030394064, -0.031858526, 0.052781112, 0.07924095, 0.0069334647, -0.2780961, 0.09244968, -0.032641098, -0.026583068, 0.036451172, 0.03341323, -0.025228117, -0.07656543, -0.118221, -0.10734222, 0.03260397, 0.15275688, 0.05852104, 0.106173486, -0.05881508, -0.082549885, 0.1174997, 0.06174134, -0.08423522, -0.055344716, -0.16970553, -0.06645013, 0.006280372, -0.11624798, -0.00490635, 0.1332938, -0.05824645, -0.00758351, -0.1255734, 0.039873965, 0.07710664, 0.045426518, -0.18234527, -0.024939088, -0.10234514, 0.08368705, -0.06852132, -0.07534842, -0.15418817, -0.14619642, -0.003937893, -0.101910375, -0.030932281, 0.1487361, -0.090863645, -0.320359, 0.09530128, -0.022416687, 0.22749043, -0.17629756, 0.21058744, -0.03136628, 0.1173821, 0.12045498, 0.03843337, 0.11156459, 0.058297314, 0.104958236, -0.11261302, ...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev = df_dev.progress_apply(vectorize_words, axis=1)\n",
    "df_dev.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(FEATURES_PATH + \"/df_train.pkl\", df_train)\n",
    "save_pickle(FEATURES_PATH + \"/df_dev.pkl\", df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_usage()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6200837,
     "sourceId": 10062018,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
