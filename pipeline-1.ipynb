{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10191619,"sourceType":"datasetVersion","datasetId":6296943},{"sourceId":10194460,"sourceType":"datasetVersion","datasetId":6299051},{"sourceId":10208028,"sourceType":"datasetVersion","datasetId":6308767},{"sourceId":10208046,"sourceType":"datasetVersion","datasetId":6308779},{"sourceId":10219934,"sourceType":"datasetVersion","datasetId":6317740},{"sourceId":10220210,"sourceType":"datasetVersion","datasetId":6317941},{"sourceId":10221151,"sourceType":"datasetVersion","datasetId":6318559},{"sourceId":10221559,"sourceType":"datasetVersion","datasetId":6318863}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# General\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport re\nimport os\nimport tqdm\nimport gc\nimport spacy\n# Load spacy model for lemmatization\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import regexp_tokenize\n\n# Models\n\nimport torch\nfrom nltk.tokenize import regexp_tokenize\n# Evaluation metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:36:53.941891Z","iopub.execute_input":"2024-12-17T03:36:53.942261Z","iopub.status.idle":"2024-12-17T03:36:54.562543Z","shell.execute_reply.started":"2024-12-17T03:36:53.942230Z","shell.execute_reply":"2024-12-17T03:36:54.561808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_train  = pd.read_parquet('/kaggle/input/data-95/train_95.parquet')\n# print(df_train.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:36:54.563911Z","iopub.execute_input":"2024-12-17T03:36:54.564196Z","iopub.status.idle":"2024-12-17T03:36:54.568163Z","shell.execute_reply.started":"2024-12-17T03:36:54.564169Z","shell.execute_reply":"2024-12-17T03:36:54.567257Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Read Json data and convert it to parqet format","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n\n# # Load JSON dataset\n# df = pd.read_json('/kaggle/input/nlp-pizzaa-ner-dataset/PIZZA_train.json')\n\n# # Save as Parquet\n# df.to_parquet('/kaggle/working/PIZZA_train_all.parquet', index=False)\n\n# print(\"JSON converted to Parquet!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:36:54.569308Z","iopub.execute_input":"2024-12-17T03:36:54.569965Z","iopub.status.idle":"2024-12-17T03:36:54.585565Z","shell.execute_reply.started":"2024-12-17T03:36:54.569920Z","shell.execute_reply":"2024-12-17T03:36:54.584716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tqdm.tqdm.pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:36:54.587137Z","iopub.execute_input":"2024-12-17T03:36:54.587400Z","iopub.status.idle":"2024-12-17T03:36:54.597066Z","shell.execute_reply.started":"2024-12-17T03:36:54.587352Z","shell.execute_reply":"2024-12-17T03:36:54.596287Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Split Test and Train Data","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.model_selection import train_test_split\n\n# # Load your dataset\n# df = pd.read_parquet('/kaggle/input/train-parquet/train.parquet')\n\n# # Shuffle the dataset\n# df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# # Define features and target\n# X = df['src']  \n# Y = df['top']  \n\n# # Split into train and test sets\n# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.05, random_state=42)\n\n# print(\"Data shuffled and split into training and testing sets.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:36:54.597982Z","iopub.execute_input":"2024-12-17T03:36:54.598209Z","iopub.status.idle":"2024-12-17T03:36:54.607168Z","shell.execute_reply.started":"2024-12-17T03:36:54.598186Z","shell.execute_reply":"2024-12-17T03:36:54.606425Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save Test and Train data","metadata":{}},{"cell_type":"code","source":"# # Combine X_train and y_train into a single DataFrame\n# df_train = pd.DataFrame({'src': X_train, 'top': y_train})\n# df_test = pd.DataFrame({'src': X_test, 'top': y_test})\n\n# # Save to the dataset folder\n# df_train.to_parquet('/kaggle/working/train_95.parquet', index=False)\n# df_test.to_parquet('/kaggle/working/test_5.parquet', index=False)\n\n# print(\"Train and test datasets saved as Parquet files.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:36:54.608103Z","iopub.execute_input":"2024-12-17T03:36:54.608350Z","iopub.status.idle":"2024-12-17T03:36:54.617187Z","shell.execute_reply.started":"2024-12-17T03:36:54.608326Z","shell.execute_reply":"2024-12-17T03:36:54.616433Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenize SRC and save it","metadata":{}},{"cell_type":"code","source":"\n# # df_train = pd.read_parquet(\"train.parquet\")\n# token_pattern=r\"(?u)\\b\\w+(?:'\\w+)?(?:-\\w+)*\\b\"\n# df_train[\"tokenized\"] = df_train[\"src\"].progress_apply(lambda x: regexp_tokenize(x, token_pattern)) \n# # print(df_train[\"tokenized\"])\n# df_train.to_parquet('/kaggle/working/train_95.parquet', index=False)\n# print(\"Tokenized src data saved to parquet file.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:36:54.618113Z","iopub.execute_input":"2024-12-17T03:36:54.618402Z","iopub.status.idle":"2024-12-17T03:36:54.626992Z","shell.execute_reply.started":"2024-12-17T03:36:54.618338Z","shell.execute_reply":"2024-12-17T03:36:54.626199Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tranform BIO tags to numbers","metadata":{}},{"cell_type":"code","source":"# # from sklearn.preprocessing import LabelEncoder\n# full_text = \" \".join(df_train['top'].to_list())\n# entities = [x.group() for x in re.finditer(\"(?<=\\()[A-Z]+(_[A-Z]+)*\", full_text)]\n# entities = list(set(entities)) # Unique\n\n#  # Using BIO Tagging\n# bio_entities = [f\"{letter}-{entity}\" for entity in entities for letter in \"BI\"]\n# bio_entities.append('O')\n# bio_entities\n\n# # label_encoder = LabelEncoder()\n# # label_encoder.fit(bio_entities)\n# print(len(bio_entities))\n# print(bio_entities)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:36:54.627974Z","iopub.execute_input":"2024-12-17T03:36:54.628296Z","iopub.status.idle":"2024-12-17T03:36:54.638049Z","shell.execute_reply.started":"2024-12-17T03:36:54.628269Z","shell.execute_reply":"2024-12-17T03:36:54.637385Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extract TOP Target IS and Save it ","metadata":{}},{"cell_type":"code","source":"# def extract_labels(top: str, entities):\n#     # Extract words and parenthesis\n#     pattern = r\"\\b\\w+(?:'\\w+)?(?:-\\w+)*\\b|[()]\"\n#     tokens = regexp_tokenize(top, pattern)\n    \n#     labels = []\n#     count = 0\n  \n#     is_beginning = True\n#     order_type = \"PIZZAORDER\"\n#     for i, token in enumerate(tokens):\n       \n#         if token in entities and token not in [\"PIZZAORDER\", \"DRINKORDER\"]:\n#             continue\n#         elif token == \"(\":\n#             count += 1\n#         elif token == \")\":\n#             count -= 1\n#         elif token == \"PIZZAORDER\":\n#             order_type = \"PIZZAORDER\"\n#         elif token == \"DRINKORDER\":\n#             order_type = \"DRINKORDER\"\n        \n#         elif count == 0:\n#             labels.append(\"O\")\n#             is_beginning = True\n#         else:\n#             if is_beginning == True:\n#                 labels.append(\"B-\" + order_type)\n#                 is_beginning = False\n#                 continue\n#             if is_beginning == False:\n#                 labels.append(\"I-\" + order_type)\n#     labels = label_encoder.transform(labels)\n#     return labels\n\n\n# # Extract one sample from the 'train.TOP' column (for example, the first entry)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:36:54.638995Z","iopub.execute_input":"2024-12-17T03:36:54.639245Z","iopub.status.idle":"2024-12-17T03:36:54.651270Z","shell.execute_reply.started":"2024-12-17T03:36:54.639220Z","shell.execute_reply":"2024-12-17T03:36:54.650355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to apply extract_labels to each row\ndef apply_extract_labels(row, entities):\n    # Extract labels for each row's 'top' column\n    labels = extract_labels(row['top'], entities)\n    \n    # Return the labels (make sure it's in a proper format for storing in DataFrame)\n    return labels.tolist()\n\n# # Apply the function to each row in the 'top' column and store the result in a new column 'IS_labels'\n# df_train['IS_labels'] = df_train.apply(lambda row: apply_extract_labels(row, entities), axis=1)\n\n# # Check the result\n\n# print(df_train[['top', 'IS_labels']].head())\n\n# # Save the modified DataFrame to a new Parquet file\n# df_train.to_parquet('train_95.parquet', index=False)\n\n# print(\"Data with IS_labels saved to train_95\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:36:54.653893Z","iopub.execute_input":"2024-12-17T03:36:54.654146Z","iopub.status.idle":"2024-12-17T03:36:54.661309Z","shell.execute_reply.started":"2024-12-17T03:36:54.654120Z","shell.execute_reply":"2024-12-17T03:36:54.660427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(df_train[['src','tokenized','top', 'IS_labels']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:36:54.662399Z","iopub.execute_input":"2024-12-17T03:36:54.662672Z","iopub.status.idle":"2024-12-17T03:36:54.674285Z","shell.execute_reply.started":"2024-12-17T03:36:54.662647Z","shell.execute_reply":"2024-12-17T03:36:54.673604Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# convert parquet to csv","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n\n# # Read the Parquet file\n\n\n# # Save it as a CSV file\n# df.to_csv('/kaggle/working/train_95.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:36:54.675939Z","iopub.execute_input":"2024-12-17T03:36:54.676291Z","iopub.status.idle":"2024-12-17T03:36:54.685061Z","shell.execute_reply.started":"2024-12-17T03:36:54.676250Z","shell.execute_reply":"2024-12-17T03:36:54.684252Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# word2vec","metadata":{}},{"cell_type":"code","source":"DATASET_PATH = \"/kaggle/input/pizza-dataset\"\nOUTPUT_ROOT_PATH = \"/kaggle/working\"\nMODELS_PATH = OUTPUT_ROOT_PATH + \"/models\"\nPYTORCH_MODELS_PATH = MODELS_PATH + \"/checkpoints\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:36:54.685862Z","iopub.execute_input":"2024-12-17T03:36:54.686088Z","iopub.status.idle":"2024-12-17T03:36:54.694253Z","shell.execute_reply.started":"2024-12-17T03:36:54.686064Z","shell.execute_reply":"2024-12-17T03:36:54.693599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(MODELS_PATH, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:36:54.695202Z","iopub.execute_input":"2024-12-17T03:36:54.695501Z","iopub.status.idle":"2024-12-17T03:36:54.705049Z","shell.execute_reply.started":"2024-12-17T03:36:54.695464Z","shell.execute_reply":"2024-12-17T03:36:54.704008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_train = pd.read_parquet(\"train.parquet\")\n# print(df_train['tokenized'][0])\n# print(df_train['tokenized'].apply(type).value_counts())\n# print(df_train.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:36:54.706356Z","iopub.execute_input":"2024-12-17T03:36:54.706668Z","iopub.status.idle":"2024-12-17T03:36:54.715239Z","shell.execute_reply.started":"2024-12-17T03:36:54.706643Z","shell.execute_reply":"2024-12-17T03:36:54.714520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = pd.read_parquet('/kaggle/input/data-95/train_95.parquet',columns=['tokenized'])\nsentences=df_train['tokenized'].tolist()\n\n\n# Convert each numpy array in the list to a Python list\nsentences = [sentence.tolist() if isinstance(sentence, np.ndarray) else sentence for sentence in sentences]\n'''  \n<class 'list'>\n<class 'list'>\n<class 'str'>\n['party', 'size', 'dried', 'peppers', 'pizza', 'and', 'a', 'sprite']\n'''\n# Verify the format\nprint(type(sentences))         # Should be list\nprint(type(sentences[0]))      # Should be list\nprint(type(sentences[0][0]))   # Should be str\nprint(sentences[0])            # Check the first sentence\n\n\n\n# Train Word2Vec model\nword2vec_model = Word2Vec(sentences=sentences, vector_size=200, window=5, min_count=1, workers=4)\nprint(word2vec_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:36:54.716273Z","iopub.execute_input":"2024-12-17T03:36:54.716622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# max_length = max(len(sentence) for sentence in df_train['IS_labels']) \n# print(max_length) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n\n# # Specify the file path you want to delete\n# file_path = '/kaggle/working/word2vec_features.npy'  # Replace with the path of the file you want to delete\n\n# # Check if the file exists before deleting it\n# if os.path.exists(file_path):\n#     os.remove(file_path)\n#     print(f\"File {file_path} has been deleted.\")\n# else:\n#     print(f\"File {file_path} not found.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nDATASET_PATH = \"/kaggle/input/pizza-dataset\"\nOUTPUT_ROOT_PATH = \"/kaggle/working\"\nMODELS_PATH = OUTPUT_ROOT_PATH + \"/models\"\nPYTORCH_MODELS_PATH = MODELS_PATH + \"/checkpoints\"\n\n# General\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport re\nimport os\nimport tqdm\n\nimport spacy\n# Load spacy model for lemmatization\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import regexp_tokenize\n\n# Models\n\nimport torch\n\n# Evaluation metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # File paths\n# parquet_file_path = '/kaggle/working/train_95.parquet'\n# model_file_path = '/kaggle/input/word2vecmodel/word2vec_model.bin'\n\n# # Load the Parquet file\n# train_data = pd.read_parquet(parquet_file_path)\n# print(\"Parquet file loaded successfully.\")\n\n# # Load the Word2Vec model\n# word2vec_model = Word2Vec.load(model_file_path)\n# print(\"Word2Vec model loaded successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# show variables taking much space in the ram","metadata":{}},{"cell_type":"code","source":"# import sys\n# import pandas as pd\n\n# # Create a snapshot of the global namespace to prevent changes during iteration\n# global_vars = globals().copy()\n\n# # Create a list to store variable information\n# variables = []\n\n# # Loop through all variables in the global namespace\n# for var_name, var_value in global_vars.items():\n#     try:\n#         size = sys.getsizeof(var_value)\n#         variables.append({'Variable': var_name, 'Size (Bytes)': size})\n#     except Exception as e:\n#         variables.append({'Variable': var_name, 'Size (Bytes)': 'Unknown'})\n\n# # Convert to a DataFrame for easy viewing\n# variables_df = pd.DataFrame(variables).sort_values(by='Size (Bytes)', ascending=False)\n\n# # Show top 10 largest variables\n# print(variables_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # List of variables you want to delete\n# vars_to_delete = ['df', 'train_df', 'df_train', 'Y', 'y_train', 'full_text', 'X_train', 'X', 'df_test','y_test','X_test','variables_df']\n\n# # Delete each variable in the list\n# for var_name in vars_to_delete:\n#     if var_name in globals():\n#         del globals()[var_name]\n\n# # Verify that only the 'train_data' variable remains\n# print(globals().keys())  # This should only show 'train_data'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# show memory consumption","metadata":{}},{"cell_type":"code","source":"# import psutil\n\n# # Get the current system memory status\n# memory = psutil.virtual_memory()\n\n# # Display total memory, used memory, and available memory\n# print(f\"Total Memory: {memory.total / (1024 ** 3):.2f} GB\")\n# print(f\"Used Memory: {memory.used / (1024 ** 3):.2f} GB\")\n# print(f\"Free Memory: {memory.available / (1024 ** 3):.2f} GB\")\n# print(f\"Memory Percentage Used: {memory.percent}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Load tokenized data (assuming you have 'tokenized' column in the train.parquet)\n# train_data = pd.read_parquet(\"train.parquet\", columns=[\"tokenized\", \"IS_labels\"])\n# train_data['tokenized'] = train_data['tokenized'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n# Apply the get_word_vectors function to get word vectors for each sentence\n# train_data['word_vectors'] = train_data['tokenized'].apply(lambda x: get_word_vectors(x, word2vec_model)) \n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"markdown","source":"## checkpoint\n","metadata":{}},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, epoch, loss, file_path):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss\n    }\n    torch.save(checkpoint, file_path)\n    print(f\"Checkpoint saved at {file_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the data","metadata":{}},{"cell_type":"markdown","source":"## Define LSTM in pytorch","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.2):\n        super(LSTMModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        # LSTM layer\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n        # Fully connected layer for classification\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        # Pass input through LSTM\n        lstm_out, _ = self.lstm(x)\n        # Use the output of the last time step for classification\n        out = self.fc(lstm_out)  # Shape: (batch_size, output_size)\n        return out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define DataSet Class","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport torch\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import Word2Vec\n\nclass CustomDataSet(Dataset):\n    def __init__(self, file,model):\n        df_train = pd.read_parquet(file,columns=['tokenized','IS_labels'])\n        self.sentences = df_train['tokenized']\n        self.labels = df_train['IS_labels']\n        self.feature_exctractor_model = model\n       \n\n    def __len__(self):\n        return len(self.labels)\n        \n    def get_sentence_vectors(self, sentence):\n        \"\"\"\n        Convert a tokenized sentence into a list of word vectors using the Word2Vec model.\n        \n        :param sentence: List of tokens.\n        :return: List of word vectors (numpy array).\n        \"\"\"\n        sentence_vectors = []\n        for word in sentence:\n            if word in self.feature_exctractor_model.wv.key_to_index:\n                sentence_vectors.append(self.feature_exctractor_model.wv[word])  # Word2Vec vector\n            else:\n                sentence_vectors.append(np.zeros(self.vector_size))  # Zero vector for unknown words\n        # print(\"Sentence vectors:\", sentence_vectors[:5])  # Displaying a few vectors\n        return sentence_vectors\n\n    def __getitem__(self, idx):\n        # Get the sentence vector\n        feature = self.get_sentence_vectors(self.sentences.iloc[idx])\n        # Apply padding to the sentence vectors\n        padded_feature = pad_sequences([feature], maxlen=50, dtype='float32', padding='post')\n        \n        # For sequence labels, pad them as well\n        label = self.labels.iloc[idx]\n        padded_label = pad_sequences([label], maxlen=50, padding='post', value=-1)  # Padding with -1 for labels\n        return padded_feature[0], padded_label[0]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Loop","metadata":{}},{"cell_type":"code","source":"import torch\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import Word2Vec\n\n# Assuming CustomDataSet and LSTMModel are defined as you provided\n\ndef save_checkpoint(model, optimizer, epoch, loss, file_path):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss\n    }\n    torch.save(checkpoint, file_path)\n    print(f\"Checkpoint saved at {file_path}\")\n\n# Training loop\ndef train_model(file,feature_extractor ,model, num_epochs=10, batch_size=32, learning_rate=0.001, chunk_size=10000):\n    # Load the dataset\n    dataset = CustomDataSet(file,feature_extractor )\n    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    # Model, loss function, and optimizer initialization\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Assuming -1 is the padding index\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Training loop\n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        total_correct_train = 0\n        total_samples_train = 0\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n\n        model.train()\n        \n        for batch_idx, (sentences_batch, labels_batch) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", unit=\"batch\")):\n            # Move batch data to the appropriate device\n            sentences_batch = sentences_batch.to(device)\n            labels_batch = labels_batch.to(device)\n            # print(\"sentences batch size :\",sentences_batch.shape)\n            # print(\"labels_batch batch size :\",labels_batch.shape)\n            \n           \n            # Forward pass\n            outputs = model(sentences_batch)  # Shape: (batch_size, seq_len, num_classes)\n            \n            # Flatten the output and labels for CrossEntropyLoss\n            outputs_flat = torch.flatten(outputs, start_dim=0, end_dim=1)  # (batch_size * seq_len, num_classes)\n            targets_flat = labels_batch.view(-1).long()  # (batch_size * seq_len)\n            # print(\"output size :\",outputs_flat.shape)\n            # print(\"targets_flat size :\",targets_flat.shape)\n            # Compute loss\n            batch_loss = criterion(outputs_flat, targets_flat)\n            optimizer.zero_grad()\n            batch_loss.backward()\n            optimizer.step()\n\n            # Accumulate loss\n            epoch_loss += batch_loss.item()\n\n            # Accuracy calculation\n            predictions = outputs.argmax(-1)  # (batch_size, seq_len)\n            mask = labels_batch != -1  # Exclude padding from accuracy calculation\n            total_correct_train += (predictions[mask] == labels_batch[mask]).sum().item()\n            total_samples_train += mask.sum().item()  # Count valid labels (non-padding)\n\n        # Calculate average loss and accuracy\n        epoch_loss /= len(train_loader)  # Average loss per batch\n        epoch_acc = total_correct_train / total_samples_train  # Accuracy: correct / total valid labels\n\n        print(f'Epoch [{epoch + 1}/{num_epochs}] | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.4f}')\n        \n        # Save the model after every epoch\n        save_checkpoint(model, optimizer, epoch, epoch_loss, f\"model_epoch_{epoch+1}.pth\")\n\n        # Clear GPU cache and collect garbage\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    print(\"Training complete!\")\n\n# Example usage:\n# Assuming you have a pre-trained Word2Vec model named `word2vec_model`\n# word2vec_model \n# # TODO : CHANGE THIS 24 TO 5 :   B-PIZZA ,I-PIZZA ,B-DRINK , I-DRINK , O\n# train_model(\"/kaggle/input/data-95/train_95.parquet\",word2vec_model ,LSTMModel(input_size=200, hidden_size=128, output_size=24), num_epochs=10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NER","metadata":{}},{"cell_type":"code","source":"\n# #full_entities = np.load('/kaggle/input/ner-entities/full_entities.npy')\n# # Print the data or its shape\n# print(\"full entities :\")\n# print(full_entities)\n\n\n# # Using BIO Tagging\n# full_bio_entities = [f\"{letter}-{entity}\" for entity in full_entities for letter in \"BI\"]\n# full_bio_entities.append('O')\n# full_bio_entities.append('B-COMPLEX_QUANTITY')\n# full_bio_entities.append('I-COMPLEX_QUANTITY')\n# full_bio_entities.remove('I-PIZZAORDER')\n# full_bio_entities.remove('B-PIZZAORDER')\n# full_bio_entities.remove('B-DRINKORDER')\n# full_bio_entities.remove('I-DRINKORDER')\n# label_encoder = LabelEncoder()\n# label_encoder.fit(full_bio_entities)\n\n# print(full_bio_entities)\n# def extract_NER_labels(top: str, entities):\n#     # Extract words and parenthesis\n#     pattern = r\"\\b\\w+(?:'\\w+)?(?:-\\w+)*\\b|[()]\"\n#     tokens = regexp_tokenize(top, pattern)\n    \n#     labels = []\n#     count = 0\n#     not_str =\"\"\n#     complex_topping_begin = False \n#     is_beginning = True\n#     order_type = \"\"\n#     for token in tokens:\n#         if token in [\"PIZZAORDER\", \"DRINKORDER\",'ORDER']:\n#             count -= 1\n#             continue\n#         elif token == \"(\":\n#             count += 1\n#         elif token == \")\":\n#             count -= 1\n#             if count == 0:\n#                 is_beginning = True\n#                 complex_topping_begin = False\n#                 not_str = \"\"\n#                 order_type = \"\"\n#             if count < 0:\n#                 count = 0\n#         elif token == \"COMPLEX_TOPPING\":\n#             order_type = \"COMPLEX_TOPPING\"\n#             complex_topping_begin = True\n#         elif token == \"NOT\":\n#             not_str = \"NOT_\"\n#         elif token in entities:\n#             order_type = token\n#         elif count == 0:\n#             labels.append(\"O\")\n           \n#         else:\n#             if complex_topping_begin:\n#                 if is_beginning:\n#                     labels.append(\"B-COMPLEX_\" + order_type)\n#                     is_beginning = False\n#                     continue\n#                 else:\n#                     labels.append(\"I-COMPLEX_\" + order_type)\n#             if is_beginning:\n#                 labels.append(\"B-\" + not_str + order_type)\n#                 is_beginning = False\n#                 continue\n#             else:\n#                 labels.append(\"I-\" + not_str + order_type) \n#     labels = label_encoder.transform(labels)\n#     return labels\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Function to apply extract_labels to each row\n# def apply_extract_ner_labels(row, entities):\n#     # Extract labels for each row's 'top' column\n#     labels = extract_NER_labels(row['top'], entities)\n    \n#     # Return the labels (make sure it's in a proper format for storing in DataFrame)\n#     return labels\n\n# # # Apply the function to each row in the 'top' column and store the result in a new column 'IS_labels'\n# df_train = pd.read_parquet('/kaggle/input/data-95/train_95.parquet')\n# df_train['NER_labels'] = df_train.apply(lambda row: apply_extract_ner_labels(row, full_entities), axis=1)\n# print(df_train.head())\n# # Check the result\n\n\n\n# # Save the modified DataFrame to a new Parquet file\n# #df_train.to_parquet('train_95.parquet', index=False)\n\n# #print(\"Data with IS_labels saved to train_95\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(len(full_bio_entities))\n# df_train.to_parquet('/kaggle/working/train_95_with_ner.parquet', index=False)\n# # ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_train = pd.read_parquet('/kaggle/working/train_95_with_ner.parquet') \n# print(df_train.columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# row_values = df_train[['top', 'NER_labels','src','tokenized']].iloc[0]\n# print(row_values['top'])         # Value from 'top'\n# print(row_values['NER_labels'])  # Value from 'NER_labels'\n# print(row_values['tokenized'])  # Value from 'NER_labels'\n# print(df_train[['top', 'NER_labels']].head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NER DataSet","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport torch\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import Word2Vec\n\nclass CustomDataSet(Dataset):\n    def __init__(self, file,model):\n        df_train = pd.read_parquet(file,columns=['tokenized','NER_labels'])\n        self.sentences = df_train['tokenized']\n        self.labels = df_train['NER_labels']\n        self.feature_exctractor_model = model\n       \n\n    def __len__(self):\n        return len(self.labels)\n        \n    def get_sentence_vectors(self, sentence):\n        \"\"\"\n        Convert a tokenized sentence into a list of word vectors using the Word2Vec model.\n        \n        :param sentence: List of tokens.\n        :return: List of word vectors (numpy array).\n        \"\"\"\n        sentence_vectors = []\n        for word in sentence:\n            if word in self.feature_exctractor_model.wv.key_to_index:\n                sentence_vectors.append(self.feature_exctractor_model.wv[word])  # Word2Vec vector\n            else:\n                sentence_vectors.append(np.zeros(self.vector_size))  # Zero vector for unknown words\n        # print(\"Sentence vectors:\", sentence_vectors[:5])  # Displaying a few vectors\n        return sentence_vectors\n\n    def __getitem__(self, idx):\n        # Get the sentence vector\n        feature = self.get_sentence_vectors(self.sentences.iloc[idx])\n        # Apply padding to the sentence vectors\n        padded_feature = pad_sequences([feature], maxlen=50, dtype='float32', padding='post')\n        \n        # For sequence labels, pad them as well\n        label = self.labels.iloc[idx]\n        padded_label = pad_sequences([label], maxlen=50, padding='post', value=-1)  # Padding with -1 for labels\n        return padded_feature[0], padded_label[0]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Loop NER","metadata":{}},{"cell_type":"code","source":"import torch\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import Word2Vec\n\n# Assuming CustomDataSet and LSTMModel are defined as you provided\n\ndef save_checkpoint(model, optimizer, epoch, loss, file_path):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss\n    }\n    torch.save(checkpoint, file_path)\n    print(f\"Checkpoint saved at {file_path}\")\n\n# Training loop\ndef train_model(file,feature_extractor ,model, num_epochs=10, batch_size=32, learning_rate=0.001, chunk_size=10000):\n    # Load the dataset\n    dataset = CustomDataSet(file,feature_extractor )\n    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    # Model, loss function, and optimizer initialization\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Assuming -1 is the padding index\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Training loop\n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        total_correct_train = 0\n        total_samples_train = 0\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n\n        model.train()\n        \n        for batch_idx, (sentences_batch, labels_batch) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", unit=\"batch\")):\n            # Move batch data to the appropriate device\n            sentences_batch = sentences_batch.to(device)\n            labels_batch = labels_batch.to(device)\n            # print(\"sentences batch size :\",sentences_batch.shape)\n            # print(\"labels_batch batch size :\",labels_batch.shape)\n            \n           \n            # Forward pass\n            outputs = model(sentences_batch)  # Shape: (batch_size, seq_len, num_classes)\n            \n            # Flatten the output and labels for CrossEntropyLoss\n            outputs_flat = torch.flatten(outputs, start_dim=0, end_dim=1)  # (batch_size * seq_len, num_classes)\n            targets_flat = labels_batch.view(-1).long()  # (batch_size * seq_len)\n            # print(\"output size :\",outputs_flat.shape)\n            # print(\"targets_flat size :\",targets_flat.shape)\n            # Compute loss\n            batch_loss = criterion(outputs_flat, targets_flat)\n            optimizer.zero_grad()\n            batch_loss.backward()\n            optimizer.step()\n\n            # Accumulate loss\n            epoch_loss += batch_loss.item()\n\n            # Accuracy calculation\n            predictions = outputs.argmax(-1)  # (batch_size, seq_len)\n            mask = labels_batch != -1  # Exclude padding from accuracy calculation\n            total_correct_train += (predictions[mask] == labels_batch[mask]).sum().item()\n            total_samples_train += mask.sum().item()  # Count valid labels (non-padding)\n\n        # Calculate average loss and accuracy\n        epoch_loss /= len(train_loader)  # Average loss per batch\n        epoch_acc = total_correct_train / total_samples_train  # Accuracy: correct / total valid labels\n\n        print(f'Epoch [{epoch + 1}/{num_epochs}] | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.4f}')\n        \n        # Save the model after every epoch\n        save_checkpoint(model, optimizer, epoch, epoch_loss, f\"model_epoch_{epoch+1}.pth\")\n\n        # Clear GPU cache and collect garbage\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    print(\"Training complete!\")\n\n# Example usage:\n# Assuming you have a pre-trained Word2Vec model named `word2vec_model`\nword2vec_model \ntrain_model(\"/kaggle/input/dataset-95-with-ner-labels/train_95_with_ner.parquet\",word2vec_model ,LSTMModel(input_size=200, hidden_size=128, output_size=25), num_epochs=10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}