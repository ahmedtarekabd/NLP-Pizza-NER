{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10220210,"sourceType":"datasetVersion","datasetId":6317941},{"sourceId":10221151,"sourceType":"datasetVersion","datasetId":6318559},{"sourceId":10228384,"sourceType":"datasetVersion","datasetId":6323908},{"sourceId":10228640,"sourceType":"datasetVersion","datasetId":6324124},{"sourceId":10229037,"sourceType":"datasetVersion","datasetId":6324405},{"sourceId":10229425,"sourceType":"datasetVersion","datasetId":6324700},{"sourceId":10229470,"sourceType":"datasetVersion","datasetId":6324736},{"sourceId":10229672,"sourceType":"datasetVersion","datasetId":6324871}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# General\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport re\nimport os\nimport tqdm\nimport gc\nimport spacy\n# Load spacy model for lemmatization\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import regexp_tokenize\n\n# Models\n\nimport torch\nfrom nltk.tokenize import regexp_tokenize\n# Evaluation metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:39:23.360346Z","iopub.execute_input":"2024-12-17T22:39:23.360715Z","iopub.status.idle":"2024-12-17T22:39:23.935562Z","shell.execute_reply.started":"2024-12-17T22:39:23.360686Z","shell.execute_reply":"2024-12-17T22:39:23.934859Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"# General\nimport numpy as np\nimport pandas as pd\n\n# Preprocessing\nfrom gensim.models import FastText\nimport os\nimport json\nimport pickle\nimport psutil\nimport numpy as np\nimport pandas as pd\nimport tqdm\nfrom typing import Literal\n\n#* Configurations\n#* Folder Paths\nDATASET_PATH = \"../../data/dataset\" # Local\nDATASET_PATH = \"/kaggle/input/dataset-with-is-final-labels/train_95_with_NER_and_IS_labels.parquet\" # Kaggle\n\nOUTPUT_ROOT_PATH = \"../../data/saved\" # Local\nOUTPUT_ROOT_PATH = \"/kaggle/working\" # Kaggle\n\nPROCESSED_DATA_PATH = OUTPUT_ROOT_PATH + \"/data\"\nFEATURES_PATH = OUTPUT_ROOT_PATH + \"/features\"\nMODELS_PATH = OUTPUT_ROOT_PATH + \"/models\"\n\n#* Common Variables\ntoken_pattern=r\"(?u)\\b\\w+(?:'\\w+)?(?:-\\w+)*\\b\"\n\ndef run_config():\n    #* Pandas\n    pd.set_option('display.max_colwidth', 1000) # Show all content of the cells\n    # pd.reset_option('display.max_colwidth') # Undo with \n    \n    #* Config tqdm for pandas\n    tqdm.tqdm.pandas()\n\n    #* Output Folders\n    os.makedirs(OUTPUT_ROOT_PATH, exist_ok=True)\n    os.makedirs(FEATURES_PATH, exist_ok=True)\n    os.makedirs(MODELS_PATH, exist_ok=True)\n    os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n    # os.rmdir(OUTPUT_ROOT_PATH)\n    # os.rmdir(FEATURES_PATH)\n    # os.rmdir(MODELS_PATH)\n    # os.rmdir(PROCESSED_DATA_PATH)\nrun_config()\n\nimport os\nimport json\nimport pickle\nimport pyarrow as pa\nimport pyarrow.dataset as pda\nimport pyarrow.parquet as pq\nimport glob\nimport psutil\nimport numpy as np\nimport pandas as pd\nfrom typing import Literal\n\n\ntypes = Literal[\"model\", \"feature\", \"processed\"]\n\n#* General\ndef file_exists(path):\n    return os.path.exists(path)\n\ndef add_to_path(path: str, type: types | None = None):\n    if type is not None:\n        if type == \"model\":\n            path = MODELS_PATH + \"/\" + path\n        elif type == \"feature\":\n            path = FEATURES_PATH + \"/\" + path\n        elif type == \"processed\":\n            path = PROCESSED_DATA_PATH + \"/\" + path\n    return path\n\n#* Memory Management & Performance\ndef memory_usage():\n    process = psutil.Process(os.getpid())\n    return (process.memory_info().rss / 1024 ** 2)\n\n\n#* Save & Load functions\ndef save_pickle(path: str, obj, type: types | None = None):\n    path = add_to_path(path, type)\n    with open (path, 'wb') as f:\n        pickle.dump(obj, f)\n\ndef load_pickle(path: str, type: types | None = None):\n    path = add_to_path(path, type)\n    with open(path, 'rb') as f:\n        return pickle.load(f)\n    \ndef save_parquet(path: str, obj, type: types | None = None):\n    path = add_to_path(path, type)\n    obj.to_parquet(path, engine='pyarrow', compression='snappy')\n\ndef load_parquet(path: str, type: types | None = None):\n    path = add_to_path(path, type)\n    return pd.read_parquet(path, engine='pyarrow')\n    \ndef save_np(path: str, obj, type: types | None = None, allow_pickle=True):\n    path = add_to_path(path, type)\n    np.save(path, obj, allow_pickle=allow_pickle)\n\ndef load_np(path: str, type: types | None = None, allow_pickle=True):\n    path = add_to_path(path, type)\n    return np.load(path, allow_pickle=allow_pickle)\n\ndef save_dict_to_json(path: str, obj, type: types | None = None):\n    path = add_to_path(path, type)\n    # Convert ndarray to list\n    for key, value in obj.items():\n        if isinstance(value, np.ndarray):\n            obj[key] = value.tolist()\n\n    with open(path, 'w') as f:\n        json.dump(obj, f)\n\ndef load_json_to_dict(path: str, type: types | None = None):\n    path = add_to_path(path, type)\n    with open(path, 'r') as f:\n        return json.load(f)\n\ndef load_json(filename: str, cols: list[str] | None = None):\n    \"\"\"\n    Load a json file into a pandas DataFrame.\n    * This function is useful (for some reason) for loading the large dataset files.\n    \n    filename: str\n        The name of the file to load.\n    cols: list[str] | None\n        The columns to load. If None, load all columns.\n    return: pd.DataFrame\n        The DataFrame containing the data from the json file.\n    \"\"\"\n    all_cols = True if cols is None else False\n    data = []\n\n    with open(filename, encoding='latin-1') as f:\n        line = f.readline()\n        f.seek(0) # Go back to the beginning of the file\n        doc = json.loads(line)\n        if all_cols:\n            cols = list(doc.keys())\n        \n        for line in f:\n            doc = json.loads(line)\n            lst = [doc[col] for col in cols]\n            data.append(lst)\n\n    df = pd.DataFrame(data=data, columns=cols)\n    return df\n\n\ndef process_parquet_in_chunks(input_file: str, output_file: str, chunk_size: int, preprocess_function: callable, args: tuple = (), merge_chunks: bool=True):\n    \"\"\"\n    Process a large Parquet file in chunks, applying a preprocessing function to each row, \n    and save the processed chunks as new Parquet files. Optionally merge the processed chunks.\n    Source: https://blog.clairvoyantsoft.com/efficient-processing-of-parquet-files-in-chunks-using-pyarrow-b315cc0c62f9\n\n    Parameters:\n    - input_file (str): Path to the input Parquet file.\n    - output_file (str): Path to save the processed Parquet file.\n    - chunk_size (int): Number of rows to process per chunk.\n    - preprocess_function (function): Function to apply to each row.\n    - merge_chunks (bool): Whether to merge the processed chunks into a single Parquet file (default: True).\n\n    Returns:\n    - None\n    \"\"\"\n\n    parquet_file = pq.ParquetFile(input_file) # Dataframe which does not fit into system memory\n\n    for i, batch in enumerate(parquet_file.iter_batches(batch_size=chunk_size)):\n        df = batch.to_pandas()\n        # Process the chunk (batch)\n        processed_chunk = df.progress_apply(preprocess_function, args=args, axis=1)\n\n        # Save the processed chunk to a new Parquet file\n        output_chunk = f\"{output_file}_{i}.parquet\"\n        processed_chunk.to_parquet(output_chunk, engine='pyarrow', compression='snappy')\n        print(f\"Chunk {i} processed and saved to {output_chunk}\")\n\n    # Optionally merge processed chunks\n    if merge_chunks:\n        print(\"Merging processed chunks into a single Parquet file...\")\n\n        # Get all processed chunk files\n        parquet_files = glob.glob(f\"{output_file}_*.parquet\")\n        # Read and concatenate them into a single DataFrame\n        final_df = pd.concat([pd.read_parquet(file) for file in parquet_files], ignore_index=True)\n        # Save the final DataFrame as a single Parquet file\n        final_df.to_parquet(output_file, engine='pyarrow', compression='snappy')\n        # Remove the processed chunk files\n        for file in parquet_files:\n            os.remove(file)\n\n        print(f\"Merged file saved to {output_file}\")\n\n\ndef process_pickles_in_chunks(input_file: str, output_file: str, chunk_size: int, preprocess_function: callable, args: tuple = (), merge_chunks: bool=True):\n    \"\"\"\n    Process a large pickle file in chunks, applying a preprocessing function to each row, \n    and save the processed chunks as new pickle files. Optionally merge the processed chunks.\n\n    Parameters:\n    - input_file (str): Path to the input pickle file.\n    - output_file (str): Path to save the processed pickle file.\n    - chunk_size (int): Number of rows to process per chunk.\n    - preprocess_function (function): Function to apply to each row.\n    - merge_chunks (bool): Whether to merge the processed chunks into a single pickle file (default: True).\n\n    Returns:\n    - None\n    \"\"\"\n\n    # Load the pickle file\n    with open(input_file, 'rb') as f:\n        data = pickle.load(f)\n\n    # Split the data into chunks\n    chunks = [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n\n    for i, chunk in enumerate(chunks):\n        # Process the chunk\n        processed_chunk = [preprocess_function(row, *args) for row in chunk]\n\n        # Save the processed chunk to a new pickle file\n        output_chunk = f\"{output_file}_{i}.pkl\"\n        with open(output_chunk, 'wb') as f:\n            pickle.dump(processed_chunk, f)\n\n        print(f\"Chunk {i} processed and saved to {output_chunk}\")\n\n    # Optionally merge processed chunks\n    if merge_chunks:\n        print(\"Merging processed chunks into a single pickle file...\")\n\n        # Get all processed chunk files\n        pickle_files = glob.glob(f\"{output_file}_*.pkl\")\n        # Read and concatenate them into a single list\n        final_data = []\n        for file in pickle_files:\n            with open(file, 'rb') as f:\n                final_data.extend(pickle.load(f))\n        # Save the final list as a single pickle file\n        with open(output_file, 'wb') as f:\n            pickle.dump(final_data, f)\n        # Remove the processed chunk files\n        for file in pickle_files:\n            os.remove(file)\n\n        print(f\"Merged file saved to {output_file}\")\n\nX_train = pd.read_parquet(DATASET_PATH)\nX_train[:5]\n\nfrom gensim.models import FastText\nmodel_name = \"/fast_text_model.bin\"\nupdate_model = True\nif update_model or not os.path.exists(MODELS_PATH + model_name):\n    print(f\"Creating '{model_name}'...\")\n    # Create a FastText model\n    EMBED_SIZE = 300\n    fast_text_model = FastText(sentences=X_train, vector_size=EMBED_SIZE, window=5, min_count=1, workers=4)\n    # fast_text_model.wv.add_vector(\"<UNK>\", np.zeros(EMBED_SIZE))\n    # fast_text_model.wv[\"<PAD>\"] = np.zeros(EMBED_SIZE)\n\n    # Save the trained model\n    print(f\"Saving '{model_name}'...\")\n    fast_text_model.save(MODELS_PATH + model_name)\nelse:\n    print(f\"Loading '{model_name}'...\")\n    # Load the trained model\n    fast_text_model = FastText.load(MODELS_PATH + model_name)\n    \nfast_text_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"FEATURE_EXTRACTOR = 'fasttext'\nPIPELINE = 'IS'\nOUTPUT_SIZE = 5 if PIPELINE == 'IS' else 25","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:42.086956Z","iopub.execute_input":"2024-12-17T22:00:42.087289Z","iopub.status.idle":"2024-12-17T22:00:48.290235Z","shell.execute_reply.started":"2024-12-17T22:00:42.087253Z","shell.execute_reply":"2024-12-17T22:00:48.289338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train  = pd.read_parquet('/kaggle/input/dataset-with-is-final-labels/train_95_with_NER_and_IS_labels.parquet')\nprint(df_train.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:48.291319Z","iopub.execute_input":"2024-12-17T22:00:48.291660Z","iopub.status.idle":"2024-12-17T22:00:54.967315Z","shell.execute_reply.started":"2024-12-17T22:00:48.291619Z","shell.execute_reply":"2024-12-17T22:00:54.966417Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Read Json data and convert it to parqet format","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n\n# # Load JSON dataset\n# df = pd.read_json('/kaggle/input/nlp-pizzaa-ner-dataset/PIZZA_train.json')\n\n# # Save as Parquet\n# df.to_parquet('/kaggle/working/PIZZA_train_all.parquet', index=False)\n\n# print(\"JSON converted to Parquet!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:54.969958Z","iopub.execute_input":"2024-12-17T22:00:54.970664Z","iopub.status.idle":"2024-12-17T22:00:54.974309Z","shell.execute_reply.started":"2024-12-17T22:00:54.970622Z","shell.execute_reply":"2024-12-17T22:00:54.973509Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tqdm.tqdm.pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:54.975490Z","iopub.execute_input":"2024-12-17T22:00:54.976063Z","iopub.status.idle":"2024-12-17T22:00:54.985628Z","shell.execute_reply.started":"2024-12-17T22:00:54.976021Z","shell.execute_reply":"2024-12-17T22:00:54.984708Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# remove ORDER(","metadata":{}},{"cell_type":"code","source":"# df_train = pd.read_parquet('/kaggle/input/dataset-95-with-ner-and-is-labels/train_95_with_NER_and_IS_labels.parquet')\n# print(df_train['src'].head())\n\n\n\n# df_train['top'] = df_train['top'].str.replace(r\"^\\(ORDER\\s?\", \"\", regex=True)\n# df_train['top'] = df_train['top'].str.replace(r\"\\)$\", \"\", regex=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:54.986752Z","iopub.execute_input":"2024-12-17T22:00:54.987548Z","iopub.status.idle":"2024-12-17T22:00:54.996979Z","shell.execute_reply.started":"2024-12-17T22:00:54.987507Z","shell.execute_reply":"2024-12-17T22:00:54.996145Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Split Test and Train Data","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.model_selection import train_test_split\n\n# # Load your dataset\n# df = pd.read_parquet('/kaggle/input/train-parquet/train.parquet')\n\n# # Shuffle the dataset\n# df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# # Define features and target\n# X = df['src']  \n# Y = df['top']  \n\n# # Split into train and test sets\n# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.05, random_state=42)\n\n# print(\"Data shuffled and split into training and testing sets.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:54.997946Z","iopub.execute_input":"2024-12-17T22:00:54.998182Z","iopub.status.idle":"2024-12-17T22:00:55.007875Z","shell.execute_reply.started":"2024-12-17T22:00:54.998159Z","shell.execute_reply":"2024-12-17T22:00:55.007149Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save Test and Train data","metadata":{}},{"cell_type":"code","source":"# # Combine X_train and y_train into a single DataFrame\n# df_train = pd.DataFrame({'src': X_train, 'top': y_train})\n# df_test = pd.DataFrame({'src': X_test, 'top': y_test})\n\n# # Save to the dataset folder\n# df_train.to_parquet('/kaggle/working/train_95.parquet', index=False)\n# df_test.to_parquet('/kaggle/working/test_5.parquet', index=False)\n\n# print(\"Train and test datasets saved as Parquet files.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.008774Z","iopub.execute_input":"2024-12-17T22:00:55.009086Z","iopub.status.idle":"2024-12-17T22:00:55.018729Z","shell.execute_reply.started":"2024-12-17T22:00:55.009052Z","shell.execute_reply":"2024-12-17T22:00:55.018029Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenize SRC and save it","metadata":{}},{"cell_type":"code","source":"\n# # df_train = pd.read_parquet(\"train.parquet\")\n# token_pattern=r\"(?u)\\b\\w+(?:'\\w+)?(?:\\s*-\\s*\\w+)*\\b\"\n# df_train[\"tokenized\"] = df_train[\"src\"].progress_apply(lambda x: regexp_tokenize(x, token_pattern)) \n# # print(df_train[\"tokenized\"])\n# df_train.to_parquet('/kaggle/working/train_95_with_NER_and_IS_labels.parquet', index=False)\n# print(\"Tokenized src data saved to parquet file.\")\n# print(df_train['tokenized'].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.019704Z","iopub.execute_input":"2024-12-17T22:00:55.019943Z","iopub.status.idle":"2024-12-17T22:00:55.032151Z","shell.execute_reply.started":"2024-12-17T22:00:55.019919Z","shell.execute_reply":"2024-12-17T22:00:55.031495Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tranform BIO tags to numbers","metadata":{}},{"cell_type":"code","source":"# # # from sklearn.preprocessing import LabelEncoder\n# full_text = \" \".join(df_train['top'].to_list())\n# entities = [x.group() for x in re.finditer(\"(?<=\\()[A-Z]+(_[A-Z]+)*\", full_text)]\n# entities = list(set(entities)) # Unique\n\n#  # Using BIO Tagging\n# bio_entities = [f\"{letter}-{entity}\" for entity in entities for letter in \"BI\"]\n# bio_entities.append('O')\n# bio_entities\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.035738Z","iopub.execute_input":"2024-12-17T22:00:55.036127Z","iopub.status.idle":"2024-12-17T22:00:55.041388Z","shell.execute_reply.started":"2024-12-17T22:00:55.036102Z","shell.execute_reply":"2024-12-17T22:00:55.040692Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"IS_BIO_entities = [\n    'B-PIZZAORDER',\n    'I-PIZZAORDER',\n    'B-DRINKORDER',\n    'I-DRINKORDER',\n    'O'\n] \n\nIS_label_encoder = LabelEncoder()\nIS_label_encoder.fit(IS_BIO_entities)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.042205Z","iopub.execute_input":"2024-12-17T22:00:55.042413Z","iopub.status.idle":"2024-12-17T22:00:55.060749Z","shell.execute_reply.started":"2024-12-17T22:00:55.042392Z","shell.execute_reply":"2024-12-17T22:00:55.059889Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extract TOP Target IS and Save it ","metadata":{}},{"cell_type":"code","source":"def extract_labels_IS(top: str, entities):\n    # Extract words and parenthesis\n   \n    token_pattern = r\"\\b\\w+(?:'\\w+)?(?:-\\w+)*\\b|[()]\"\n    # token_pattern=r\"(?u)\\b\\w+(?:'\\w+)?(?:\\s*-\\s*\\w+)*\\b\"\n    tokens = regexp_tokenize(top, token_pattern)\n    labels = []\n    count = 0\n  \n    is_beginning = True\n    order_type = \"PIZZAORDER\"\n    for i, token in enumerate(tokens):\n       \n        if token in entities and token not in [\"PIZZAORDER\", \"DRINKORDER\"]:\n            continue \n      \n        elif token == \"(\":\n            count += 1\n        elif token == \")\":\n            count -= 1\n        elif token == \"PIZZAORDER\":\n            order_type = \"PIZZAORDER\"\n        elif token == \"DRINKORDER\":\n            order_type = \"DRINKORDER\"\n        \n        elif count == 0:\n            labels.append(\"O\")\n            is_beginning = True\n        else:\n            if is_beginning == True:\n                labels.append(\"B-\" + order_type)\n                is_beginning = False\n                continue\n            if is_beginning == False:\n                labels.append(\"I-\" + order_type)\n    labels = IS_label_encoder.transform(labels)\n    return labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.061708Z","iopub.execute_input":"2024-12-17T22:00:55.062397Z","iopub.status.idle":"2024-12-17T22:00:55.071716Z","shell.execute_reply.started":"2024-12-17T22:00:55.062357Z","shell.execute_reply":"2024-12-17T22:00:55.070912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Function to apply extract_labels to each row\n# df_train = pd.read_parquet('/kaggle/input/final-95-train-set/train_95_with_NER_and_IS_labels (1).parquet')\n# def apply_extract_labels(row, entities):\n#     labels = extract_labels_IS(row['top'], entities)\n#     return labels.tolist()\n\n# # # Apply the function to each row in the 'top' column and store the result in a new column 'IS_labels'\n# df_train['IS_labels'] = df_train.progress_apply(lambda row: apply_extract_labels(row,['PIZZAORDER','DRINKORDER'] ), axis=1)\n\n\n# print(df_train[['top', 'IS_labels']].head())\n# labels = df_train['IS_labels']\n# print(IS_label_encoder.inverse_transform(labels[0]))\n\n# # # Save the modified DataFrame to a new Parquet file\n# df_train.to_parquet('/kaggle/working/train_95_with_NER_and_IS_labels.parquet', index=False)\n# print(\"Data with IS_labels saved to train_95\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.072669Z","iopub.execute_input":"2024-12-17T22:00:55.072926Z","iopub.status.idle":"2024-12-17T22:00:55.085274Z","shell.execute_reply.started":"2024-12-17T22:00:55.072902Z","shell.execute_reply":"2024-12-17T22:00:55.084519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_train = pd.read_parquet('/kaggle/input/final-95-train-set/train_95_with_NER_and_IS_labels (1).parquet')\n# # Ensure the values are integers and remove NaN\n# unique_values = df_train['IS_labels'].explode().dropna().unique()\n\n# # # Convert unique_values to integers (if necessary)\n# unique_values = unique_values.astype(int)\n\n# # # Use inverse_transform\n# # print(\"Decoded labels:\", IS_label_encoder.inverse_transform(unique_values))\n\n# # Print unique encoded values\n# print(\"Unique values:\", unique_values) \n# df_train.to_parquet('/kaggle/working/train_95_with_NER_and_IS_labels.parquet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.086029Z","iopub.execute_input":"2024-12-17T22:00:55.086250Z","iopub.status.idle":"2024-12-17T22:00:55.095322Z","shell.execute_reply.started":"2024-12-17T22:00:55.086228Z","shell.execute_reply":"2024-12-17T22:00:55.094677Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# convert parquet to csv","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n\n# # Read the Parquet file\n\n\n# # Save it as a CSV file\n# df.to_csv('/kaggle/working/train_95.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.096229Z","iopub.execute_input":"2024-12-17T22:00:55.096492Z","iopub.status.idle":"2024-12-17T22:00:55.107880Z","shell.execute_reply.started":"2024-12-17T22:00:55.096455Z","shell.execute_reply":"2024-12-17T22:00:55.107174Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# word2vec","metadata":{}},{"cell_type":"code","source":"DATASET_PATH = \"/kaggle/input/pizza-dataset\"\nOUTPUT_ROOT_PATH = \"/kaggle/working\"\nMODELS_PATH = OUTPUT_ROOT_PATH + \"/models\"\nPYTORCH_MODELS_PATH = MODELS_PATH + \"/checkpoints\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.108805Z","iopub.execute_input":"2024-12-17T22:00:55.109709Z","iopub.status.idle":"2024-12-17T22:00:55.117864Z","shell.execute_reply.started":"2024-12-17T22:00:55.109683Z","shell.execute_reply":"2024-12-17T22:00:55.117268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# os.makedirs(MODELS_PATH, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.118891Z","iopub.execute_input":"2024-12-17T22:00:55.119166Z","iopub.status.idle":"2024-12-17T22:00:55.126981Z","shell.execute_reply.started":"2024-12-17T22:00:55.119130Z","shell.execute_reply":"2024-12-17T22:00:55.126204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_train = pd.read_parquet(\"train.parquet\")\n# print(df_train['tokenized'][0])\n# print(df_train['tokenized'].apply(type).value_counts())\n# print(df_train.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.127854Z","iopub.execute_input":"2024-12-17T22:00:55.128085Z","iopub.status.idle":"2024-12-17T22:00:55.136817Z","shell.execute_reply.started":"2024-12-17T22:00:55.128062Z","shell.execute_reply":"2024-12-17T22:00:55.136112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_train = pd.read_parquet('/kaggle/input/final-95-train-set/train_95_with_NER_and_IS_labels (1).parquet',columns=['tokenized'])\n# sentences=df_train['tokenized'].tolist()\n\n# sentences = [sentence.tolist() if isinstance(sentence, np.ndarray) else sentence for sentence in sentences]\n# '''  \n# <class 'list'>\n# <class 'list'>\n# <class 'str'>\n# ['party', 'size', 'dried', 'peppers', 'pizza', 'and', 'a', 'sprite']\n# '''\n# # Verify the format\n# print(type(sentences))         # Should be list\n# print(type(sentences[0]))      # Should be list\n# print(type(sentences[0][0]))   # Should be str\n# print(sentences[0])            # Check the first sentence\n\n\n\n# # Train Word2Vec model\n# word2vec_model = Word2Vec(sentences=sentences, vector_size=200, window=5, min_count=1, workers=4) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.137800Z","iopub.execute_input":"2024-12-17T22:00:55.138063Z","iopub.status.idle":"2024-12-17T22:00:55.146952Z","shell.execute_reply.started":"2024-12-17T22:00:55.138035Z","shell.execute_reply":"2024-12-17T22:00:55.146206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# max_length = max(len(sentence) for sentence in df_train['IS_labels']) \n# print(max_length) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.147945Z","iopub.execute_input":"2024-12-17T22:00:55.148260Z","iopub.status.idle":"2024-12-17T22:00:55.160525Z","shell.execute_reply.started":"2024-12-17T22:00:55.148224Z","shell.execute_reply":"2024-12-17T22:00:55.159891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n\n# # Define the new directory path\n# new_dir = \"/kaggle/working/feature_extractors\"\n\n# # Create the directory if it doesn't exist\n# if not os.path.exists(new_dir):\n#     os.makedirs(new_dir)\n#     print(f\"Directory created: {new_dir}\")\n# else:\n#     print(\"Directory already exists\")\n\n# word2vec_model.save(\"/kaggle/working/feature_extractors/word2vec_model.model\")\n# print(\"Word2Vec model saved successfully!\")\n# print(word2vec_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.161494Z","iopub.execute_input":"2024-12-17T22:00:55.161776Z","iopub.status.idle":"2024-12-17T22:00:55.170478Z","shell.execute_reply.started":"2024-12-17T22:00:55.161752Z","shell.execute_reply":"2024-12-17T22:00:55.169845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n\n# # Specify the file path you want to delete\n# file_path = '/kaggle/working/word2vec_features.npy'  # Replace with the path of the file you want to delete\n\n# # Check if the file exists before deleting it\n# if os.path.exists(file_path):\n#     os.remove(file_path)\n#     print(f\"File {file_path} has been deleted.\")\n# else:\n#     print(f\"File {file_path} not found.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.171318Z","iopub.execute_input":"2024-12-17T22:00:55.171543Z","iopub.status.idle":"2024-12-17T22:00:55.182853Z","shell.execute_reply.started":"2024-12-17T22:00:55.171520Z","shell.execute_reply":"2024-12-17T22:00:55.182066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nDATASET_PATH = \"/kaggle/input/pizza-dataset\"\nOUTPUT_ROOT_PATH = \"/kaggle/working\"\nMODELS_PATH = OUTPUT_ROOT_PATH + \"/models\"\nPYTORCH_MODELS_PATH = MODELS_PATH + \"/checkpoints\"\n\n# General\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport re\nimport os\nimport tqdm\n\nimport spacy\n# Load spacy model for lemmatization\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import regexp_tokenize\n\n# Models\n\nimport torch\n\n# Evaluation metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.183850Z","iopub.execute_input":"2024-12-17T22:00:55.184126Z","iopub.status.idle":"2024-12-17T22:00:55.821669Z","shell.execute_reply.started":"2024-12-17T22:00:55.184103Z","shell.execute_reply":"2024-12-17T22:00:55.820724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # File paths\n# parquet_file_path = '/kaggle/working/train_95.parquet'\n# model_file_path = '/kaggle/input/word2vecmodel/word2vec_model.bin'\n\n# # Load the Parquet file\n# train_data = pd.read_parquet(parquet_file_path)\n# print(\"Parquet file loaded successfully.\")\n\n# # Load the Word2Vec model\n# word2vec_model = Word2Vec.load(model_file_path)\n# print(\"Word2Vec model loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.822951Z","iopub.execute_input":"2024-12-17T22:00:55.823248Z","iopub.status.idle":"2024-12-17T22:00:55.827316Z","shell.execute_reply.started":"2024-12-17T22:00:55.823221Z","shell.execute_reply":"2024-12-17T22:00:55.826462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Load tokenized data (assuming you have 'tokenized' column in the train.parquet)\n# train_data = pd.read_parquet(\"train.parquet\", columns=[\"tokenized\", \"IS_labels\"])\n# train_data['tokenized'] = train_data['tokenized'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n# Apply the get_word_vectors function to get word vectors for each sentence\n# train_data['word_vectors'] = train_data['tokenized'].apply(lambda x: get_word_vectors(x, word2vec_model)) \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.828418Z","iopub.execute_input":"2024-12-17T22:00:55.828754Z","iopub.status.idle":"2024-12-17T22:00:55.840244Z","shell.execute_reply.started":"2024-12-17T22:00:55.828706Z","shell.execute_reply":"2024-12-17T22:00:55.839523Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"markdown","source":"## Define LSTM in pytorch","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.2):\n        super(LSTMModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        # LSTM layer\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n        # Fully connected layer for classification\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        # Pass input through LSTM\n        lstm_out, _ = self.lstm(x)\n        # Use the output of the last time step for classification\n        out = self.fc(lstm_out)  # Shape: (batch_size, output_size)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.841230Z","iopub.execute_input":"2024-12-17T22:00:55.841468Z","iopub.status.idle":"2024-12-17T22:00:55.855690Z","shell.execute_reply.started":"2024-12-17T22:00:55.841445Z","shell.execute_reply":"2024-12-17T22:00:55.855001Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# IS DataSet","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport torch\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import Word2Vec\n\nclass ISDataSet(Dataset):\n    def __init__(self, file, feature_extractor_model, vector_size):\n        # Load the data\n        df_train = pd.read_parquet(file, columns=['tokenized', 'IS_labels'])\n        self.sentences = df_train['tokenized']\n        self.labels = df_train['IS_labels']\n        self.feature_extractor_model = feature_extractor_model  # Word2Vec or FastText model\n        self.vector_size = vector_size  # Size of the word embeddings\n\n    def __len__(self):\n        return len(self.labels)\n       \n\n    def __len__(self):\n        return len(self.labels)\n        \n    def get_sentence_vectors(self, sentence):\n        \"\"\"\n        Convert a tokenized sentence into a list of word vectors using the Word2Vec model.\n        \n        :param sentence: List of tokens.\n        :return: List of word vectors (numpy array).\n        \"\"\"\n        sentence_vectors = []\n        for word in sentence:\n            if word in self.feature_extractor_model.wv.key_to_index:\n                sentence_vectors.append(self.feature_extractor_model.wv[word])  # Word2Vec or  Fasttext\n            else:\n                sentence_vectors.append(np.zeros(self.vector_size))  # Zero vector for unknown words\n        # print(\"Sentence vectors:\", sentence_vectors[:5])  # Displaying a few vectors\n        return sentence_vectors\n\n    def __getitem__(self, idx):\n        # Get the sentence vector\n        feature = self.get_sentence_vectors(self.sentences.iloc[idx])\n        # Apply padding to the sentence vectors\n        padded_feature = pad_sequences([feature], maxlen=50, dtype='float32', padding='post')\n        \n        # For sequence labels, pad them as well\n        label = self.labels.iloc[idx]\n        padded_label = pad_sequences([label], maxlen=50, padding='post', value=-1)  # Padding with -1 for labels\n        return padded_feature[0], padded_label[0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.856847Z","iopub.execute_input":"2024-12-17T22:00:55.857087Z","iopub.status.idle":"2024-12-17T22:00:55.870501Z","shell.execute_reply.started":"2024-12-17T22:00:55.857046Z","shell.execute_reply":"2024-12-17T22:00:55.869578Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NER Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport torch\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import Word2Vec\n\nclass NERDataSet(Dataset):\n    def __init__(self, file, feature_extractor_model, vector_size):\n        df_train = pd.read_parquet(file,columns=['tokenized','NER_labels'])\n        self.sentences = df_train['tokenized']\n        self.labels = df_train['NER_labels']\n        self.feature_exctractor_model = feature_extractor_model\n        self.vector_size = vector_size\n       \n\n    def __len__(self):\n        return len(self.labels)\n        \n    def get_sentence_vectors(self, sentence):\n        \"\"\"\n        Convert a tokenized sentence into a list of word vectors using the Word2Vec model.\n        \n        :param sentence: List of tokens.\n        :return: List of word vectors (numpy array).\n        \"\"\"\n        sentence_vectors = []\n        for word in sentence:\n            if word in self.feature_exctractor_model.wv.key_to_index:\n                sentence_vectors.append(self.feature_exctractor_model.wv[word])  # Word2Vec vector\n            else:\n                sentence_vectors.append(np.zeros(self.vector_size))  # Zero vector for unknown words\n        # print(\"Sentence vectors:\", sentence_vectors[:5])  # Displaying a few vectors\n        return sentence_vectors\n\n    def __getitem__(self, idx):\n        # Get the sentence vector\n        feature = self.get_sentence_vectors(self.sentences.iloc[idx])\n        # Apply padding to the sentence vectors\n        padded_feature = pad_sequences([feature], maxlen=50, dtype='float32', padding='post')\n        \n        # For sequence labels, pad them as well\n        label = self.labels.iloc[idx]\n        padded_label = pad_sequences([label], maxlen=50, padding='post', value=-1)  # Padding with -1 for labels\n        return padded_feature[0], padded_label[0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.877470Z","iopub.execute_input":"2024-12-17T22:00:55.877832Z","iopub.status.idle":"2024-12-17T22:00:55.907660Z","shell.execute_reply.started":"2024-12-17T22:00:55.877791Z","shell.execute_reply":"2024-12-17T22:00:55.906971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Feature Extractor","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom gensim.models import FastText\n\nif(FEATURE_EXTRACTOR == 'fasttext'):\n    feature_extractor_model  = FastText.load(\"/kaggle/working/models/fast_text_model.bin\")\n    print(\"fasttext Model loaded successfully!\") \nelse :\n    feature_extractor_model =  Word2Vec.load(\"/kaggle/input/word2vec-model/word2vec_model.model\")\n    print(\"word2vec Model loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:55.908543Z","iopub.execute_input":"2024-12-17T22:00:55.908828Z","iopub.status.idle":"2024-12-17T22:00:56.671612Z","shell.execute_reply.started":"2024-12-17T22:00:55.908804Z","shell.execute_reply":"2024-12-17T22:00:56.670809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_train = pd.read_parquet('/kaggle/input/final-95-train-set/train_95_with_NER_and_IS_labels (1).parquet')\n# # Ensure the values are integers and remove NaN\n# unique_values = df_train['IS_labels'].explode().dropna().unique()\n\n# # Convert unique_values to integers (if necessary)\n# unique_values = unique_values.astype(int)\n\n# # Use inverse_transform\n# print(\"Decoded labels:\", IS_label_encoder.inverse_transform(unique_values))\n\n# # Print unique encoded values\n# print(\"Unique values:\", unique_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:56.672777Z","iopub.execute_input":"2024-12-17T22:00:56.673117Z","iopub.status.idle":"2024-12-17T22:00:56.677880Z","shell.execute_reply.started":"2024-12-17T22:00:56.673079Z","shell.execute_reply":"2024-12-17T22:00:56.676987Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Loop","metadata":{}},{"cell_type":"code","source":"import torch\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import Word2Vec\n\n# Assuming CustomDataSet and LSTMModel are defined as you provided\n\ndef save_checkpoint(model, optimizer, epoch, loss, file_path):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss\n    }\n    torch.save(checkpoint, file_path)\n    print(f\"Checkpoint saved at {file_path}\")\n\n# Training loop\ndef train_model(file,feature_extractor ,model, num_epochs=10, batch_size=32, learning_rate=0.001, chunk_size=10000,pipeline=\"IS\",feature_extr = \"word2vec\"):\n    # Load the dataset \n    if pipeline == \"IS\":\n        dataset = ISDataSet(file, feature_extractor_model=feature_extractor ,vector_size=feature_extractor.vector_size )\n    else :\n        dataset = NERDataSet(file,feature_extractor_model=feature_extractor ,vector_size=feature_extractor.vector_size )\n        \n    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    # Model, loss function, and optimizer initialization\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss(ignore_index=-1)  # -1 is the padding index\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Training loop\n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        total_correct_train = 0\n        total_samples_train = 0\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n\n        model.train()\n        \n        for batch_idx, (sentences_batch, labels_batch) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}_{pipeline}_{feature_extr}\", unit=\"batch\")):\n            # Move batch data to the appropriate device\n            sentences_batch = sentences_batch.to(device)\n            labels_batch = labels_batch.to(device)\n            # print(\"sentences batch size :\",sentences_batch.shape)\n            # print(\"labels_batch batch size :\",labels_batch.shape)\n            \n           \n            # Forward pass\n            outputs = model(sentences_batch)  # Shape: (batch_size, seq_len, num_classes)\n            \n            # Flatten the output and labels for CrossEntropyLoss\n            outputs_flat = torch.flatten(outputs, start_dim=0, end_dim=1)  # (batch_size * seq_len, num_classes)\n            targets_flat = labels_batch.view(-1).long()  # (batch_size * seq_len)\n            # print(\"output size :\",outputs_flat.shape)\n            # print(\"targets_flat size :\",targets_flat.shape)\n            # Compute loss\n            batch_loss = criterion(outputs_flat, targets_flat)\n            optimizer.zero_grad()\n            batch_loss.backward()\n            optimizer.step()\n\n            # Accumulate loss\n            epoch_loss += batch_loss.item()\n\n            # Accuracy calculation\n            predictions = outputs.argmax(-1)  # (batch_size, seq_len)\n            mask = labels_batch != -1  # Exclude padding from accuracy calculation\n            total_correct_train += (predictions[mask] == labels_batch[mask]).sum().item()\n            total_samples_train += mask.sum().item()  # Count valid labels (non-padding)\n\n        # Calculate average loss and accuracy\n        epoch_loss /= len(train_loader)  # Average loss per batch\n        epoch_acc = total_correct_train / total_samples_train  # Accuracy: correct / total valid labels\n\n        print(f'Epoch [{epoch + 1}/{num_epochs}] | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.4f}')\n        \n        # Save the model after every epoch\n        save_checkpoint(model, optimizer, epoch, epoch_loss, f\"IS_model_epoch_{epoch+1}.pth\")\n\n        # Clear GPU cache and collect garbage\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    print(\"Training complete!\")\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:56.678806Z","iopub.execute_input":"2024-12-17T22:00:56.679040Z","iopub.status.idle":"2024-12-17T22:00:56.691671Z","shell.execute_reply.started":"2024-12-17T22:00:56.679017Z","shell.execute_reply":"2024-12-17T22:00:56.690917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NER","metadata":{}},{"cell_type":"code","source":"\n# full_entities = np.load('/kaggle/input/ner-entities/full_entities.npy')\n# # Print the data or its shape\n# print(\"full entities :\")\n# print(full_entities)\n\n\n# # Using BIO Tagging\n# full_bio_entities = [f\"{letter}-{entity}\" for entity in full_entities for letter in \"BI\"]\n# full_bio_entities.append('O')\n# full_bio_entities.append('B-COMPLEX_QUANTITY')\n# full_bio_entities.append('I-COMPLEX_QUANTITY')\n# full_bio_entities.remove('I-PIZZAORDER')\n# full_bio_entities.remove('B-PIZZAORDER')\n# full_bio_entities.remove('B-DRINKORDER')\n# full_bio_entities.remove('I-DRINKORDER')\n# label_encoder = LabelEncoder()\n# label_encoder.fit(full_bio_entities)\n\n# print(full_bio_entities)\n# def extract_NER_labels(top: str, entities):\n#     # Extract words and parenthesis\n#     token_pattern=r\"(?u)\\b\\w+(?:'\\w+)?(?:\\s*-\\s*\\w+)*\\b|[()]\"\n#     tokens = regexp_tokenize(top, token_pattern)\n    \n#     labels = []\n#     count = 0\n#     not_str =\"\"\n#     complex_topping_begin = False \n#     is_beginning = True\n#     order_type = \"\"\n#     for token in tokens:\n#         if token in [\"PIZZAORDER\", \"DRINKORDER\",'ORDER']:\n#             count -= 1\n#             continue\n#         elif token == \"(\":\n#             count += 1\n#         elif token == \")\":\n#             count -= 1\n#             if count == 0:\n#                 is_beginning = True\n#                 complex_topping_begin = False\n#                 not_str = \"\"\n#                 order_type = \"\"\n#             if count < 0:\n#                 count = 0\n#         elif token == \"COMPLEX_TOPPING\":\n#             order_type = \"COMPLEX_TOPPING\"\n#             complex_topping_begin = True\n#         elif token == \"NOT\":\n#             not_str = \"NOT_\"\n#         elif token in entities:\n#             order_type = token\n#         elif count == 0:\n#             labels.append(\"O\")\n           \n#         else:\n#             if complex_topping_begin:\n#                 if is_beginning:\n#                     labels.append(\"B-COMPLEX_\" + order_type)\n#                     is_beginning = False\n#                     continue\n#                 else:\n#                     labels.append(\"I-COMPLEX_\" + order_type)\n#                     continue\n#             if is_beginning:\n#                 labels.append(\"B-\" + not_str + order_type)\n#                 is_beginning = False\n#                 continue\n#             else:\n#                 labels.append(\"I-\" + not_str + order_type) \n#     # print(labels)\n#     labels = label_encoder.transform(labels)\n#     return labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:56.692877Z","iopub.execute_input":"2024-12-17T22:00:56.693213Z","iopub.status.idle":"2024-12-17T22:00:56.707242Z","shell.execute_reply.started":"2024-12-17T22:00:56.693176Z","shell.execute_reply":"2024-12-17T22:00:56.706514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Function to apply extract_labels to each row\n# def apply_extract_ner_labels(row, entities):\n#     # Extract labels for each row's 'top' column\n#     labels = extract_NER_labels(row['top'], entities)\n    \n#     # Return the labels (make sure it's in a proper format for storing in DataFrame)\n#     return labels\n\n# # # Apply the function to each row in the 'top' column and store the result in a new column 'IS_labels'\n# df_train = pd.read_parquet('/kaggle/input/data-95/train_95.parquet')\n# df_train['NER_labels'] = df_train.apply(lambda row: apply_extract_ner_labels(row, full_entities), axis=1)\n# print(df_train.head())\n# # Check the result\n\n\n\n# # Save the modified DataFrame to a new Parquet file\n# #df_train.to_parquet('train_95.parquet', index=False)\n\n# #print(\"Data with IS_labels saved to train_95\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:56.708053Z","iopub.execute_input":"2024-12-17T22:00:56.708293Z","iopub.status.idle":"2024-12-17T22:00:56.720968Z","shell.execute_reply.started":"2024-12-17T22:00:56.708264Z","shell.execute_reply":"2024-12-17T22:00:56.720332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(len(full_bio_entities))\n# df_train.to_parquet('/kaggle/working/train_95_with_ner.parquet', index=False)\n# # ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:56.721885Z","iopub.execute_input":"2024-12-17T22:00:56.722122Z","iopub.status.idle":"2024-12-17T22:00:56.732536Z","shell.execute_reply.started":"2024-12-17T22:00:56.722099Z","shell.execute_reply":"2024-12-17T22:00:56.731847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_train = pd.read_parquet('/kaggle/working/train_95_with_ner.parquet') \n# print(df_train.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:56.733438Z","iopub.execute_input":"2024-12-17T22:00:56.733686Z","iopub.status.idle":"2024-12-17T22:00:56.743864Z","shell.execute_reply.started":"2024-12-17T22:00:56.733663Z","shell.execute_reply":"2024-12-17T22:00:56.743100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# row_values = df_train[['top', 'NER_labels','src','tokenized']].iloc[0]\n# print(row_values['top'])         # Value from 'top'\n# print(row_values['NER_labels'])  # Value from 'NER_labels'\n# print(row_values['tokenized'])  # Value from 'NER_labels'\n# print(df_train[['top', 'NER_labels']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:56.744589Z","iopub.execute_input":"2024-12-17T22:00:56.744837Z","iopub.status.idle":"2024-12-17T22:00:56.753652Z","shell.execute_reply.started":"2024-12-17T22:00:56.744814Z","shell.execute_reply":"2024-12-17T22:00:56.752846Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Call","metadata":{}},{"cell_type":"code","source":" train_model(file=\"/kaggle/input/dataset-with-is-final-labels/train_95_with_NER_and_IS_labels.parquet\",\n             feature_extractor=feature_extractor_model ,\n             model=LSTMModel(input_size=feature_extractor_model.vector_size, \n                       hidden_size=256,\n                       output_size=OUTPUT_SIZE,\n                       num_layers=5), \n            feature_extr=FEATURE_EXTRACTOR,\n            pipeline=PIPELINE,\n            num_epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:00:56.754645Z","iopub.execute_input":"2024-12-17T22:00:56.754964Z","iopub.status.idle":"2024-12-17T22:01:10.559897Z","shell.execute_reply.started":"2024-12-17T22:00:56.754929Z","shell.execute_reply":"2024-12-17T22:01:10.558768Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing IS","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf_dev = pd.read_json('/kaggle/input/dataset-dev/PIZZA_dev.json', lines=True)\nprint(df_dev.head())\n\n# Remove rows where 'dev.PCFG_ERR' is \"true\"\ndf_dev = df_dev[df_dev['dev.PCFG_ERR'] == \"False\"]\n\n# Verify the filtering\nprint(f\"Original rows: {len(df_dev)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:01:10.560958Z","iopub.status.idle":"2024-12-17T22:01:10.561417Z","shell.execute_reply.started":"2024-12-17T22:01:10.561168Z","shell.execute_reply":"2024-12-17T22:01:10.561196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\ndf_dev['dev.TOP'] = df_dev['dev.TOP'].str.replace(r\"^\\(ORDER\\s?\", \"\", regex=True)\n\n\n\ndf_dev['dev.TOP'] = df_dev['dev.TOP'].str.replace(r\"\\)$\", \"\", regex=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:01:10.562802Z","iopub.status.idle":"2024-12-17T22:01:10.563233Z","shell.execute_reply.started":"2024-12-17T22:01:10.563009Z","shell.execute_reply":"2024-12-17T22:01:10.563031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to apply extract_labels to each row\ndef apply_extract_labels(row, entities):\n    labels = extract_labels_IS(row['dev.TOP'], entities)\n    return labels.tolist()\n\n# # Apply the function to each row in the 'top' column and store the result in a new column 'IS_labels'\ndf_dev['IS_labels'] = df_dev.progress_apply(lambda row: apply_extract_labels(row,['PIZZAORDER','DRINKORDER'] ), axis=1)\n\n\nprint(df_dev[['dev.TOP', 'IS_labels']].head())\nlabels = df_dev['IS_labels']\nprint(IS_label_encoder.inverse_transform(labels[16]))\n\n# # Save the modified DataFrame to a new Parquet file\ndf_dev.to_parquet('/kaggle/working/dev_95_with_NER_and_IS_labels.parquet', index=False)\nprint(\"Data with IS_labels saved to dev\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:01:10.564220Z","iopub.status.idle":"2024-12-17T22:01:10.564500Z","shell.execute_reply.started":"2024-12-17T22:01:10.564365Z","shell.execute_reply":"2024-12-17T22:01:10.564379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel_file_path = '/kaggle/input/word2vec-model/word2vec_model.model'\n\n# Load the Word2Vec model\nword2vec_model = Word2Vec.load(model_file_path)\nprint(\"Word2Vec model loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:01:10.566208Z","iopub.status.idle":"2024-12-17T22:01:10.566484Z","shell.execute_reply.started":"2024-12-17T22:01:10.566349Z","shell.execute_reply":"2024-12-17T22:01:10.566363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to apply extract_labels to each row\ndef apply_extract_ner_labels(row, entities):\n    # Extract labels for each row's 'top' column\n    #print(f\"Processing index: {row.name}\")\n    # print(row['dev.TOP'])\n    labels = extract_NER_labels(row['dev.TOP'], entities)\n    \n    # Return the labels (make sure it's in a proper format for storing in DataFrame)\n    return labels\n\ndf_dev['NER_labels'] = df_dev.apply(lambda row: apply_extract_ner_labels(row, full_entities), axis=1)\nprint(df_dev.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:01:10.567188Z","iopub.status.idle":"2024-12-17T22:01:10.567481Z","shell.execute_reply.started":"2024-12-17T22:01:10.567339Z","shell.execute_reply":"2024-12-17T22:01:10.567354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"token_pattern=r\"(?u)\\b\\w+(?:'\\w+)?(?:\\s*-\\s*\\w+)*\\b\"\ndf_dev[\"tokenized\"] = df_dev[\"dev.SRC\"].progress_apply(lambda x: regexp_tokenize(x, token_pattern)) \ndf_dev.to_parquet('/kaggle/working/dev_with_labels.parquet', index=False)\nprint(\"Tokenized src data saved to parquet file.\")\nprint(df_dev['tokenized'].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:01:10.568671Z","iopub.status.idle":"2024-12-17T22:01:10.568946Z","shell.execute_reply.started":"2024-12-17T22:01:10.568813Z","shell.execute_reply":"2024-12-17T22:01:10.568826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Function to Load Trained Model\ndef load_trained_model(checkpoint_path, model, optimizer=None):\n    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n    model.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    print(f\"Loaded model from {checkpoint_path}\")\n    return model\n\n\n# Evaluate on Dev Dataset\ndef evaluate_model(dev_file, model, word2vec_model, batch_size=32):\n    # Create Dataset and DataLoader\n    dev_dataset = CustomDataSet(dev_file, word2vec_model)\n    dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model.eval()  # Set model to evaluation mode\n\n    all_predictions = []\n    all_labels = []\n\n    with torch.no_grad():\n        for sentences_batch, labels_batch in dev_loader:\n            sentences_batch = sentences_batch.to(device)\n            labels_batch = labels_batch.to(device)\n\n            outputs = model(sentences_batch)\n            predictions = torch.argmax(outputs, dim=-1)  # Get class with highest score\n            \n            # Remove padding for evaluation\n            for i in range(labels_batch.size(0)):  # Batch size\n                valid_labels = labels_batch[i][labels_batch[i] != -1]\n                valid_predictions = predictions[i][:len(valid_labels)]\n                all_labels.extend(valid_labels.cpu().numpy())\n                all_predictions.extend(valid_predictions.cpu().numpy())\n\n    # Calculate Metrics\n    print(\"Classification Report:\")\n    print(classification_report(all_labels, all_predictions))\n    print(\"Accuracy Score:\", accuracy_score(all_labels, all_predictions))\n\n\n# Main Code to Load Model and Evaluate\nif __name__ == \"__main__\":\n    # File paths\n    train_word2vec_file = \"/kaggle/input/word2vec-model/word2vec_model.model\"  # Path to Word2Vec model\n    dev_file = \"/kaggle/input/dataset-dev-parquet/dev_with_labels.parquet\"  # Path to dev dataset\n    checkpoint_path = \"/kaggle/input/is-model/IS_model_epoch_5.pth\"  # Trained model checkpoint\n\n    # Load Word2Vec model\n    word2vec_model = Word2Vec.load(train_word2vec_file)\n\n    # Define model architecture\n    input_size = 200  # Should match Word2Vec vector size\n    hidden_size = 256\n    output_size = 5  # Number of output classes\n    num_layers= 5\n    lstm_model = LSTMModel(input_size, hidden_size, output_size,num_layers)\n\n    # Load trained model\n    lstm_model = load_trained_model(checkpoint_path, lstm_model)\n\n    # Evaluate on dev dataset\n    evaluate_model(dev_file, lstm_model, word2vec_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:01:10.570546Z","iopub.status.idle":"2024-12-17T22:01:10.570988Z","shell.execute_reply.started":"2024-12-17T22:01:10.570769Z","shell.execute_reply":"2024-12-17T22:01:10.570791Z"}},"outputs":[],"execution_count":null}]}