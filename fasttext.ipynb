{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing\n",
    "from gensim.models import FastText\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from typing import Literal\n",
    "\n",
    "#* Configurations\n",
    "#* Folder Paths\n",
    "DATASET_PATH = \"../../data/dataset\" # Local\n",
    "DATASET_PATH = \"/kaggle/input/dataset-with-is-final-labels/train_95_with_NER_and_IS_labels.parquet\" # Kaggle\n",
    "\n",
    "OUTPUT_ROOT_PATH = \"../../data/saved\" # Local\n",
    "OUTPUT_ROOT_PATH = \"/kaggle/working\" # Kaggle\n",
    "\n",
    "PROCESSED_DATA_PATH = OUTPUT_ROOT_PATH + \"/data\"\n",
    "FEATURES_PATH = OUTPUT_ROOT_PATH + \"/features\"\n",
    "MODELS_PATH = OUTPUT_ROOT_PATH + \"/models\"\n",
    "\n",
    "#* Common Variables\n",
    "token_pattern=r\"(?u)\\b\\w+(?:'\\w+)?(?:-\\w+)*\\b\"\n",
    "\n",
    "def run_config():\n",
    "    #* Pandas\n",
    "    pd.set_option('display.max_colwidth', 1000) # Show all content of the cells\n",
    "    # pd.reset_option('display.max_colwidth') # Undo with \n",
    "    \n",
    "    #* Config tqdm for pandas\n",
    "    tqdm.tqdm.pandas()\n",
    "\n",
    "    #* Output Folders\n",
    "    os.makedirs(OUTPUT_ROOT_PATH, exist_ok=True)\n",
    "    os.makedirs(FEATURES_PATH, exist_ok=True)\n",
    "    os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "    os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
    "    # os.rmdir(OUTPUT_ROOT_PATH)\n",
    "    # os.rmdir(FEATURES_PATH)\n",
    "    # os.rmdir(MODELS_PATH)\n",
    "    # os.rmdir(PROCESSED_DATA_PATH)\n",
    "run_config()\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as pda\n",
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "types = Literal[\"model\", \"feature\", \"processed\"]\n",
    "\n",
    "#* General\n",
    "def file_exists(path):\n",
    "    return os.path.exists(path)\n",
    "\n",
    "def add_to_path(path: str, type: types | None = None):\n",
    "    if type is not None:\n",
    "        if type == \"model\":\n",
    "            path = MODELS_PATH + \"/\" + path\n",
    "        elif type == \"feature\":\n",
    "            path = FEATURES_PATH + \"/\" + path\n",
    "        elif type == \"processed\":\n",
    "            path = PROCESSED_DATA_PATH + \"/\" + path\n",
    "    return path\n",
    "\n",
    "#* Memory Management & Performance\n",
    "def memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return (process.memory_info().rss / 1024 ** 2)\n",
    "\n",
    "\n",
    "#* Save & Load functions\n",
    "def save_pickle(path: str, obj, type: types | None = None):\n",
    "    path = add_to_path(path, type)\n",
    "    with open (path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path: str, type: types | None = None):\n",
    "    path = add_to_path(path, type)\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def save_parquet(path: str, obj, type: types | None = None):\n",
    "    path = add_to_path(path, type)\n",
    "    obj.to_parquet(path, engine='pyarrow', compression='snappy')\n",
    "\n",
    "def load_parquet(path: str, type: types | None = None):\n",
    "    path = add_to_path(path, type)\n",
    "    return pd.read_parquet(path, engine='pyarrow')\n",
    "    \n",
    "def save_np(path: str, obj, type: types | None = None, allow_pickle=True):\n",
    "    path = add_to_path(path, type)\n",
    "    np.save(path, obj, allow_pickle=allow_pickle)\n",
    "\n",
    "def load_np(path: str, type: types | None = None, allow_pickle=True):\n",
    "    path = add_to_path(path, type)\n",
    "    return np.load(path, allow_pickle=allow_pickle)\n",
    "\n",
    "def save_dict_to_json(path: str, obj, type: types | None = None):\n",
    "    path = add_to_path(path, type)\n",
    "    # Convert ndarray to list\n",
    "    for key, value in obj.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            obj[key] = value.tolist()\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(obj, f)\n",
    "\n",
    "def load_json_to_dict(path: str, type: types | None = None):\n",
    "    path = add_to_path(path, type)\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_json(filename: str, cols: list[str] | None = None):\n",
    "    \"\"\"\n",
    "    Load a json file into a pandas DataFrame.\n",
    "    * This function is useful (for some reason) for loading the large dataset files.\n",
    "    \n",
    "    filename: str\n",
    "        The name of the file to load.\n",
    "    cols: list[str] | None\n",
    "        The columns to load. If None, load all columns.\n",
    "    return: pd.DataFrame\n",
    "        The DataFrame containing the data from the json file.\n",
    "    \"\"\"\n",
    "    all_cols = True if cols is None else False\n",
    "    data = []\n",
    "\n",
    "    with open(filename, encoding='latin-1') as f:\n",
    "        line = f.readline()\n",
    "        f.seek(0) # Go back to the beginning of the file\n",
    "        doc = json.loads(line)\n",
    "        if all_cols:\n",
    "            cols = list(doc.keys())\n",
    "        \n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            lst = [doc[col] for col in cols]\n",
    "            data.append(lst)\n",
    "\n",
    "    df = pd.DataFrame(data=data, columns=cols)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_parquet_in_chunks(input_file: str, output_file: str, chunk_size: int, preprocess_function: callable, args: tuple = (), merge_chunks: bool=True):\n",
    "    \"\"\"\n",
    "    Process a large Parquet file in chunks, applying a preprocessing function to each row, \n",
    "    and save the processed chunks as new Parquet files. Optionally merge the processed chunks.\n",
    "    Source: https://blog.clairvoyantsoft.com/efficient-processing-of-parquet-files-in-chunks-using-pyarrow-b315cc0c62f9\n",
    "\n",
    "    Parameters:\n",
    "    - input_file (str): Path to the input Parquet file.\n",
    "    - output_file (str): Path to save the processed Parquet file.\n",
    "    - chunk_size (int): Number of rows to process per chunk.\n",
    "    - preprocess_function (function): Function to apply to each row.\n",
    "    - merge_chunks (bool): Whether to merge the processed chunks into a single Parquet file (default: True).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    parquet_file = pq.ParquetFile(input_file) # Dataframe which does not fit into system memory\n",
    "\n",
    "    for i, batch in enumerate(parquet_file.iter_batches(batch_size=chunk_size)):\n",
    "        df = batch.to_pandas()\n",
    "        # Process the chunk (batch)\n",
    "        processed_chunk = df.progress_apply(preprocess_function, args=args, axis=1)\n",
    "\n",
    "        # Save the processed chunk to a new Parquet file\n",
    "        output_chunk = f\"{output_file}_{i}.parquet\"\n",
    "        processed_chunk.to_parquet(output_chunk, engine='pyarrow', compression='snappy')\n",
    "        print(f\"Chunk {i} processed and saved to {output_chunk}\")\n",
    "\n",
    "    # Optionally merge processed chunks\n",
    "    if merge_chunks:\n",
    "        print(\"Merging processed chunks into a single Parquet file...\")\n",
    "\n",
    "        # Get all processed chunk files\n",
    "        parquet_files = glob.glob(f\"{output_file}_*.parquet\")\n",
    "        # Read and concatenate them into a single DataFrame\n",
    "        final_df = pd.concat([pd.read_parquet(file) for file in parquet_files], ignore_index=True)\n",
    "        # Save the final DataFrame as a single Parquet file\n",
    "        final_df.to_parquet(output_file, engine='pyarrow', compression='snappy')\n",
    "        # Remove the processed chunk files\n",
    "        for file in parquet_files:\n",
    "            os.remove(file)\n",
    "\n",
    "        print(f\"Merged file saved to {output_file}\")\n",
    "\n",
    "\n",
    "def process_pickles_in_chunks(input_file: str, output_file: str, chunk_size: int, preprocess_function: callable, args: tuple = (), merge_chunks: bool=True):\n",
    "    \"\"\"\n",
    "    Process a large pickle file in chunks, applying a preprocessing function to each row, \n",
    "    and save the processed chunks as new pickle files. Optionally merge the processed chunks.\n",
    "\n",
    "    Parameters:\n",
    "    - input_file (str): Path to the input pickle file.\n",
    "    - output_file (str): Path to save the processed pickle file.\n",
    "    - chunk_size (int): Number of rows to process per chunk.\n",
    "    - preprocess_function (function): Function to apply to each row.\n",
    "    - merge_chunks (bool): Whether to merge the processed chunks into a single pickle file (default: True).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the pickle file\n",
    "    with open(input_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # Split the data into chunks\n",
    "    chunks = [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Process the chunk\n",
    "        processed_chunk = [preprocess_function(row, *args) for row in chunk]\n",
    "\n",
    "        # Save the processed chunk to a new pickle file\n",
    "        output_chunk = f\"{output_file}_{i}.pkl\"\n",
    "        with open(output_chunk, 'wb') as f:\n",
    "            pickle.dump(processed_chunk, f)\n",
    "\n",
    "        print(f\"Chunk {i} processed and saved to {output_chunk}\")\n",
    "\n",
    "    # Optionally merge processed chunks\n",
    "    if merge_chunks:\n",
    "        print(\"Merging processed chunks into a single pickle file...\")\n",
    "\n",
    "        # Get all processed chunk files\n",
    "        pickle_files = glob.glob(f\"{output_file}_*.pkl\")\n",
    "        # Read and concatenate them into a single list\n",
    "        final_data = []\n",
    "        for file in pickle_files:\n",
    "            with open(file, 'rb') as f:\n",
    "                final_data.extend(pickle.load(f))\n",
    "        # Save the final list as a single pickle file\n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(final_data, f)\n",
    "        # Remove the processed chunk files\n",
    "        for file in pickle_files:\n",
    "            os.remove(file)\n",
    "\n",
    "        print(f\"Merged file saved to {output_file}\")\n",
    "\n",
    "X_train = pd.read_parquet(DATASET_PATH)\n",
    "X_train[:5]\n",
    "\n",
    "from gensim.models import FastText\n",
    "model_name = \"/fast_text_model.bin\"\n",
    "update_model = True\n",
    "if update_model or not os.path.exists(MODELS_PATH + model_name):\n",
    "    print(f\"Creating '{model_name}'...\")\n",
    "    # Create a FastText model\n",
    "    EMBED_SIZE = 300\n",
    "    fast_text_model = FastText(sentences=X_train, vector_size=EMBED_SIZE, window=5, min_count=1, workers=4)\n",
    "    # fast_text_model.wv.add_vector(\"<UNK>\", np.zeros(EMBED_SIZE))\n",
    "    # fast_text_model.wv[\"<PAD>\"] = np.zeros(EMBED_SIZE)\n",
    "\n",
    "    # Save the trained model\n",
    "    print(f\"Saving '{model_name}'...\")\n",
    "    fast_text_model.save(MODELS_PATH + model_name)\n",
    "else:\n",
    "    print(f\"Loading '{model_name}'...\")\n",
    "    # Load the trained model\n",
    "    fast_text_model = FastText.load(MODELS_PATH + model_name)\n",
    "    \n",
    "fast_text_model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
